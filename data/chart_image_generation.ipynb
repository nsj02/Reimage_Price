{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-Imaging Price Trends - Image Generation & Comparison Analysis\n",
    "\n",
    "**Purpose**: \n",
    "1. Generate candlestick chart images from stock price data\n",
    "2. **Detailed comparison with original author data (img_data/)**\n",
    "3. Statistical analysis and visual comparison\n",
    "4. Performance benchmarking\n",
    "\n",
    "**Next Step**: Run `2_model_training.ipynb` after completion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Environment Setup & Optimization Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "os.chdir('/content/drive/MyDrive/ReImaging_Price_Trends')\n",
    "print(f\"ğŸš€ Current directory: {os.getcwd()}\")\n",
    "print(f\"ğŸ“ Available files: {[f for f in os.listdir('.') if not f.startswith('.')]}\")\n",
    "\n",
    "# Check Numba JIT performance optimization\n",
    "try:\n",
    "    import numba\n",
    "    print(f\"\\nâš¡ Numba JIT available: {numba.__version__}\")\n",
    "    print(\"   ğŸš€ Image generation speed improved by 50-100x!\")\n",
    "except ImportError:\n",
    "    print(\"\\nâš ï¸  Numba installation failed - check requirements.txt\")\n",
    "    print(\"   ğŸŒ Using slower pure Python version\")\n",
    "\n",
    "# Check memory status\n",
    "import psutil\n",
    "memory = psutil.virtual_memory()\n",
    "print(f\"\\nğŸ’¾ Available memory: {memory.available // (1024**3):.1f}GB\")\n",
    "if memory.available < 2 * (1024**3):  # Less than 2GB\n",
    "    print(\"âš ï¸  Warning: Low memory - recommend using --parallel 1 option\")\n",
    "    \n",
    "print(f\"\\nâœ… Environment setup completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install requirements\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Availability Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Check data files availability and analyze\nprint(\"ğŸ“Š DATA AVAILABILITY & ANALYSIS\")\nprint(\"=\" * 50)\n\n# Updated to use unified dataset file\ndata_files = [\n    'data_1992_2019_unified.parquet'\n]\n\nprint(\"\\nğŸ“ˆ Required data files:\")\ndata_ready = True\ntotal_size_mb = 0\n\nfor file in data_files:\n    if os.path.exists(file):\n        size_mb = os.path.getsize(file) / (1024**2)\n        total_size_mb += size_mb\n        print(f\"   âœ… {file} ({size_mb:.1f}MB)\")\n        \n        # Analyze data content\n        try:\n            df = pd.read_parquet(file)\n            print(f\"      Records: {len(df):,}, Companies: {df['code'].nunique():,}\")\n            print(f\"      Period: {df['date'].min()} ~ {df['date'].max()}\")\n            \n            # Check data quality\n            missing_ohlc = df[['open', 'high', 'low', 'close']].isnull().any(axis=1).sum()\n            missing_volume = df['volume'].isnull().sum()\n            missing_returns = df[['ret5', 'ret20', 'ret60']].isnull().any(axis=1).sum()\n            \n            print(f\"      Missing OHLC: {missing_ohlc:,} ({missing_ohlc/len(df)*100:.1f}%)\")\n            print(f\"      Missing volume: {missing_volume:,} ({missing_volume/len(df)*100:.1f}%)\")\n            print(f\"      Missing returns: {missing_returns:,} ({missing_returns/len(df)*100:.1f}%)\")\n            \n            # Label distribution\n            for period in [5, 20, 60]:\n                label_col = f'label_{period}'\n                if label_col in df.columns:\n                    up_count = df[label_col].sum()\n                    total_valid = df[label_col].notna().sum()\n                    up_pct = up_count / total_valid * 100 if total_valid > 0 else 0\n                    print(f\"      {period}-day up: {up_count:,}/{total_valid:,} ({up_pct:.1f}%)\")\n            \n            del df  # Free memory\n            \n        except Exception as e:\n            print(f\"      âŒ Error analyzing data: {e}\")\n    else:\n        print(f\"   âŒ {file} missing\")\n        data_ready = False\n\nprint(f\"\\nğŸ“‹ Summary:\")\nprint(f\"   Data ready: {'âœ… Yes' if data_ready else 'âŒ No'}\")\nprint(f\"   Total size: {total_size_mb:.1f}MB\")\n\nif not data_ready:\n    print(f\"\\nâš ï¸  Missing data files!\")\n    print(f\"   ğŸ“ Please run: datageneration.ipynb first\")\nelse:\n    print(f\"\\nğŸ‰ All required data files available and analyzed!\")\n    print(f\"   ğŸš€ Ready to proceed with image analysis\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Image Generation - Our Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Generate 5-day images\nprint(\"ğŸ–¼ï¸  GENERATING 5-DAY IMAGES\")\nprint(\"=\" * 50)\n\nstart_time = time.time()\n\nprint(\"ğŸ“Š Training data (1993-2000):\")\n!python chart_generator.py --image_days 5 --mode train\n\nprint(\"\\nğŸ“Š Test data (2001-2019):\")\n!python chart_generator.py --image_days 5 --mode test\n\nelapsed_5d = time.time() - start_time\nprint(f\"\\nâ±ï¸  5-day generation completed in {elapsed_5d:.1f} seconds\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Generate 20-day images  \nprint(\"ğŸ–¼ï¸  GENERATING 20-DAY IMAGES\")\nprint(\"=\" * 50)\n\nstart_time = time.time()\n\nprint(\"ğŸ“Š Training data (1993-2000):\")\n!python chart_generator.py --image_days 20 --mode train\n\nprint(\"\\nğŸ“Š Test data (2001-2019):\")\n!python chart_generator.py --image_days 20 --mode test\n\nelapsed_20d = time.time() - start_time\nprint(f\"\\nâ±ï¸  20-day generation completed in {elapsed_20d:.1f} seconds\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Generate 60-day images\nprint(\"ğŸ–¼ï¸  GENERATING 60-DAY IMAGES\")\nprint(\"=\" * 50)\n\nstart_time = time.time()\n\nprint(\"ğŸ“Š Training data (1993-2000):\")\n!python chart_generator.py --image_days 60 --mode train\n\nprint(\"\\nğŸ“Š Test data (2001-2019):\")\n!python chart_generator.py --image_days 60 --mode test\n\nelapsed_60d = time.time() - start_time\nprint(f\"\\nâ±ï¸  60-day generation completed in {elapsed_60d:.1f} seconds\")\n\ntotal_elapsed = elapsed_5d + elapsed_20d + elapsed_60d\nprint(f\"\\nğŸ‰ Total generation time: {total_elapsed:.1f} seconds ({total_elapsed/60:.1f} minutes)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generated Images Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Check our generated reconstructed images\nprint(\"ğŸ“Š IMAGE DATA SUMMARY\")\nprint(\"=\" * 60)\n\n# Check available data directories\navailable_dirs = {\n    'original_author': 'img_data',\n    'reconstructed_filled': 'img_data_reconstructed_filled'\n}\n\nreconstructed_stats = {}\n\nfor version, base_dir in available_dirs.items():\n    if os.path.exists(base_dir):\n        total_images = 0\n        total_size_gb = 0\n        success_count = 0\n        \n        subdirs = ['weekly_5d', 'monthly_20d', 'quarterly_60d']\n        \n        for subdir in subdirs:\n            img_dir = os.path.join(base_dir, subdir)\n            \n            if os.path.exists(img_dir):\n                # Count .dat and .feather files\n                dat_files = [f for f in os.listdir(img_dir) if f.endswith('.dat')]\n                feather_files = [f for f in os.listdir(img_dir) if f.endswith('.feather')]\n                \n                # Calculate total size\n                dir_size = sum(os.path.getsize(os.path.join(img_dir, f)) \n                             for f in os.listdir(img_dir) if f.endswith(('.dat', '.feather')))\n                size_gb = dir_size / (1024**3)\n                total_size_gb += size_gb\n                \n                # Estimate image count (rough calculation)\n                dat_size = sum(os.path.getsize(os.path.join(img_dir, f)) for f in dat_files)\n                \n                if 'weekly_5d' in subdir:\n                    image_size = 32 * 15\n                elif 'monthly_20d' in subdir:\n                    image_size = 64 * 60\n                else:  # quarterly_60d\n                    image_size = 96 * 180\n                \n                estimated_images = dat_size // image_size\n                total_images += estimated_images\n                success_count += 1\n                \n                print(f\"âœ… {version} {subdir:15}: {estimated_images:8,} images, {len(dat_files)} .dat, {len(feather_files)} .feather\")\n        \n        reconstructed_stats[version] = {\n            'images': total_images,\n            'size_gb': total_size_gb,\n            'success_count': success_count\n        }\n        \n        print(f\"\\nğŸ“‹ {version.upper()} SUMMARY:\")\n        print(f\"   Success rate: {success_count}/3 ({success_count/3*100:.0f}%)\")\n        print(f\"   Total images: {total_images:,}\")\n        print(f\"   Total size: {total_size_gb:.2f}GB\")\n    else:\n        print(f\"âŒ {version}: Directory {base_dir} not found\")\n        reconstructed_stats[version] = None\n\nif reconstructed_stats.get('original_author') and reconstructed_stats.get('reconstructed_filled'):\n    print(f\"\\nğŸ‰ Both datasets available for comparison!\")\nelif reconstructed_stats.get('reconstructed_filled'):\n    print(f\"\\nâœ… Reconstructed filled data available for training\")\nelse:\n    print(f\"\\nâš ï¸  No image data found - need to generate images first\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ğŸ” DETAILED COMPARISON: Original Author vs Our Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Check original author data availability\nprint(\"ğŸ” ORIGINAL AUTHOR DATA AVAILABILITY CHECK\")\nprint(\"=\" * 60)\n\noriginal_base_dir = 'img_data'\noriginal_available = False\n\nif os.path.exists(original_base_dir):\n    print(f\"\\nğŸ“‚ Found original author data: {original_base_dir}/\")\n    original_available = True\n    \n    subdirs = ['weekly_5d', 'monthly_20d', 'quarterly_60d'] \n    original_total_images = 0\n    original_total_size_gb = 0\n    original_success_count = 0\n\n    for subdir in subdirs:\n        img_dir = os.path.join(original_base_dir, subdir)\n\n        if os.path.exists(img_dir):\n            # Check .dat and .feather files\n            dat_files = [f for f in os.listdir(img_dir) if f.endswith('.dat')]\n            feather_files = [f for f in os.listdir(img_dir) if f.endswith('.feather')]\n\n            # Calculate total size\n            dir_size = sum(os.path.getsize(os.path.join(img_dir, f)) \n                         for f in os.listdir(img_dir) if f.endswith(('.dat', '.feather')))\n            size_gb = dir_size / (1024**3)\n            original_total_size_gb += size_gb\n\n            # Estimate image count\n            dat_size = sum(os.path.getsize(os.path.join(img_dir, f)) for f in dat_files)\n            \n            if 'weekly_5d' in subdir:\n                image_size = 32 * 15\n            elif 'monthly_20d' in subdir:\n                image_size = 64 * 60\n            else:  # quarterly_60d\n                image_size = 96 * 180\n            \n            estimated_images = dat_size // image_size\n            original_total_images += estimated_images\n            original_success_count += 1\n            \n            print(f\"âœ… {subdir:15}: {estimated_images:8,} images, {size_gb:6.2f}GB\")\n        else:\n            print(f\"âŒ {subdir:15}: directory missing\")\n    \n    if original_success_count > 0:\n        print(f\"\\nğŸ“Š ORIGINAL AUTHOR SUMMARY:\")\n        print(f\"   Available directories: {original_success_count}/3\")\n        print(f\"   Total images: {original_total_images:,}\")\n        print(f\"   Total size: {original_total_size_gb:.2f}GB\")\n    else:\n        print(f\"\\nâŒ No valid original author data found\")\n        original_available = False\n        \nelse:\n    print(f\"\\nâŒ Original author data not found: {original_base_dir}/\")\n    print(f\"   ğŸ“ Note: Original data not available for comparison\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side-by-side comparison analysis\n",
    "if original_available and reconstructed_stats:\n",
    "    print(\"ğŸ“Š SIDE-BY-SIDE COMPARISON ANALYSIS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    print(f\"\\n{'Directory':<15} {'Original':<20} {'Reconstructed':<20} {'Ratio':<10} {'Status':<10}\")\n",
    "    print(\"-\" * 85)\n",
    "    \n",
    "    comparison_results = {}\n",
    "    total_original_images = 0\n",
    "    total_reconstructed_images = 0\n",
    "    \n",
    "    for dir_name in reconstructed_dirs.keys():\n",
    "        orig_data = original_stats.get(dir_name)\n",
    "        recon_data = reconstructed_stats.get(dir_name)\n",
    "        \n",
    "        if orig_data and recon_data:\n",
    "            orig_images = orig_data['images']\n",
    "            recon_images = recon_data['images']\n",
    "            ratio = recon_images / orig_images if orig_images > 0 else 0\n",
    "            \n",
    "            total_original_images += orig_images\n",
    "            total_reconstructed_images += recon_images\n",
    "            \n",
    "            # Status assessment\n",
    "            if ratio >= 0.95:\n",
    "                status = \"âœ… Good\"\n",
    "            elif ratio >= 0.8:\n",
    "                status = \"âš ï¸  OK\"\n",
    "            else:\n",
    "                status = \"âŒ Poor\"\n",
    "            \n",
    "            comparison_results[dir_name] = {\n",
    "                'original': orig_images,\n",
    "                'reconstructed': recon_images,\n",
    "                'ratio': ratio,\n",
    "                'status': status\n",
    "            }\n",
    "            \n",
    "            print(f\"{dir_name:<15} {orig_images:8,} images   {recon_images:8,} images   {ratio:8.3f}  {status:<10}\")\n",
    "        \n",
    "        elif orig_data:\n",
    "            print(f\"{dir_name:<15} {orig_data['images']:8,} images   {'N/A':<15}  {'N/A':<8}  {'âŒ Missing':<10}\")\n",
    "        elif recon_data:\n",
    "            print(f\"{dir_name:<15} {'N/A':<15}  {recon_data['images']:8,} images   {'N/A':<8}  {'âŒ No Orig':<10}\")\n",
    "        else:\n",
    "            print(f\"{dir_name:<15} {'N/A':<15}  {'N/A':<15}  {'N/A':<8}  {'âŒ Both':<10}\")\n",
    "    \n",
    "    # Overall comparison\n",
    "    if total_original_images > 0 and total_reconstructed_images > 0:\n",
    "        overall_ratio = total_reconstructed_images / total_original_images\n",
    "        \n",
    "        print(f\"\\nğŸ¯ OVERALL COMPARISON:\")\n",
    "        print(f\"   Original total: {total_original_images:,} images\")\n",
    "        print(f\"   Reconstructed total: {total_reconstructed_images:,} images\")\n",
    "        print(f\"   Overall ratio: {overall_ratio:.3f}\")\n",
    "        print(f\"   Difference: {total_reconstructed_images - total_original_images:+,} images\")\n",
    "        \n",
    "        # Assessment\n",
    "        if overall_ratio >= 0.95:\n",
    "            assessment = \"ğŸ‰ Excellent reconstruction! Very close to original.\"\n",
    "        elif overall_ratio >= 0.8:\n",
    "            assessment = \"âœ… Good reconstruction with minor differences.\"\n",
    "        elif overall_ratio >= 0.6:\n",
    "            assessment = \"âš ï¸  Moderate reconstruction - significant data loss detected.\"\n",
    "        else:\n",
    "            assessment = \"âŒ Poor reconstruction - major data loss detected!\"\n",
    "        \n",
    "        print(f\"\\nğŸ’¡ Assessment: {assessment}\")\n",
    "        \n",
    "        # Detailed insights\n",
    "        print(f\"\\nğŸ” Insights:\")\n",
    "        if overall_ratio < 0.9:\n",
    "            print(f\"   ğŸ“‰ Reconstruction has {(1-overall_ratio)*100:.1f}% fewer images\")\n",
    "            print(f\"   ğŸ” Possible causes:\")\n",
    "            print(f\"      - More strict NA filtering in our implementation\")\n",
    "            print(f\"      - Different IPO/delisting filtering logic\")\n",
    "            print(f\"      - Different sampling rate or window selection\")\n",
    "            print(f\"   ğŸ“ Recommend: Review filtering logic in create_original_format.py\")\n",
    "        elif overall_ratio > 1.1:\n",
    "            print(f\"   ğŸ“ˆ Reconstruction has {(overall_ratio-1)*100:.1f}% more images\")\n",
    "            print(f\"   ğŸ” Possible causes:\")\n",
    "            print(f\"      - Less strict filtering in our implementation\")\n",
    "            print(f\"      - Different handling of missing data\")\n",
    "            print(f\"   ğŸ“ This might actually be beneficial for training\")\n",
    "        else:\n",
    "            print(f\"   ğŸ¯ Image counts are very similar - good reconstruction quality\")\n",
    "            print(f\"   âœ… Filtering logic appears to match the original paper\")\n",
    "\n",
    "elif not original_available:\n",
    "    print(\"\\nâš ï¸  COMPARISON NOT POSSIBLE\")\n",
    "    print(\"   ğŸ“ Original author data not available\")\n",
    "    print(\"   ğŸ’¡ To enable comparison:\")\n",
    "    print(f\"      1. Obtain original img_data/ from paper authors\")\n",
    "    print(f\"      2. Place in project root directory\")\n",
    "    print(f\"      3. Re-run this analysis\")\n",
    "    \n",
    "elif not reconstructed_stats:\n",
    "    print(\"\\nâŒ RECONSTRUCTION FAILED\")\n",
    "    print(\"   ğŸ“ Our reconstruction was not successful\")\n",
    "    print(\"   ğŸ’¡ Check previous cells for error messages\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\nâŒ UNEXPECTED ERROR\")\n",
    "    print(\"   ğŸ“ Both datasets appear to be missing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visual Sample Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display sample images for visual comparison\n",
    "if original_available and reconstructed_stats:\n",
    "    print(\"ğŸ–¼ï¸  VISUAL SAMPLE COMPARISON\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Focus on monthly_20d for visual comparison\n",
    "    focus_dir = 'monthly_20d'\n",
    "    \n",
    "    original_dir = os.path.join('img_data', focus_dir)\n",
    "    reconstructed_dir = os.path.join('img_data_reconstructed', focus_dir)\n",
    "    \n",
    "    if os.path.exists(original_dir) and os.path.exists(reconstructed_dir):\n",
    "        # Find common files\n",
    "        original_files = set(os.listdir(original_dir))\n",
    "        reconstructed_files = set(os.listdir(reconstructed_dir))\n",
    "        common_dat_files = [f for f in original_files.intersection(reconstructed_files) \n",
    "                           if f.endswith('.dat')]\n",
    "        \n",
    "        if common_dat_files:\n",
    "            sample_file = common_dat_files[0]  # Take first common .dat file\n",
    "            print(f\"\\nğŸ“Š Comparing sample file: {sample_file}\")\n",
    "            \n",
    "            # Load sample images\n",
    "            def load_sample_images(dat_path, num_samples=3, image_height=64, image_width=60):\n",
    "                \"\"\"Load sample images from .dat file\"\"\"\n",
    "                images = []\n",
    "                image_size = image_height * image_width\n",
    "                \n",
    "                try:\n",
    "                    with open(dat_path, 'rb') as f:\n",
    "                        file_size = os.path.getsize(dat_path)\n",
    "                        max_images = file_size // image_size\n",
    "                        num_to_load = min(num_samples, max_images)\n",
    "                        \n",
    "                        for i in range(num_to_load):\n",
    "                            f.seek(i * image_size)\n",
    "                            img_data = f.read(image_size)\n",
    "                            if len(img_data) == image_size:\n",
    "                                img_array = np.frombuffer(img_data, dtype=np.uint8)\n",
    "                                img_2d = img_array.reshape(image_height, image_width)\n",
    "                                images.append(img_2d)\n",
    "                except Exception as e:\n",
    "                    print(f\"   âŒ Error loading {dat_path}: {e}\")\n",
    "                    return []\n",
    "                \n",
    "                return images\n",
    "            \n",
    "            # Load samples from both datasets\n",
    "            original_images = load_sample_images(os.path.join(original_dir, sample_file))\n",
    "            reconstructed_images = load_sample_images(os.path.join(reconstructed_dir, sample_file))\n",
    "            \n",
    "            if original_images and reconstructed_images:\n",
    "                # Create visualization\n",
    "                num_samples = min(len(original_images), len(reconstructed_images), 3)\n",
    "                \n",
    "                fig, axes = plt.subplots(2, num_samples, figsize=(5*num_samples, 10))\n",
    "                fig.suptitle(f'Visual Comparison: Original vs Reconstructed\\n({sample_file})', \n",
    "                            fontsize=16, fontweight='bold')\n",
    "                \n",
    "                for i in range(num_samples):\n",
    "                    # Original images (top row)\n",
    "                    axes[0, i].imshow(original_images[i], cmap='gray', aspect='auto')\n",
    "                    axes[0, i].set_title(f'Original #{i+1}', fontweight='bold', color='blue')\n",
    "                    axes[0, i].set_xlabel('Trading Days (3 pixels each)')\n",
    "                    axes[0, i].set_ylabel('Price Level')\n",
    "                    \n",
    "                    # Reconstructed images (bottom row)\n",
    "                    axes[1, i].imshow(reconstructed_images[i], cmap='gray', aspect='auto')\n",
    "                    axes[1, i].set_title(f'Reconstructed #{i+1}', fontweight='bold', color='red')\n",
    "                    axes[1, i].set_xlabel('Trading Days (3 pixels each)')\n",
    "                    axes[1, i].set_ylabel('Price Level')\n",
    "                    \n",
    "                    # Add statistics annotations\n",
    "                    orig_nonzero = np.count_nonzero(original_images[i])\n",
    "                    recon_nonzero = np.count_nonzero(reconstructed_images[i])\n",
    "                    \n",
    "                    axes[0, i].text(0.02, 0.98, f'White pixels: {orig_nonzero}', \n",
    "                                   transform=axes[0, i].transAxes, verticalalignment='top',\n",
    "                                   bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))\n",
    "                    \n",
    "                    axes[1, i].text(0.02, 0.98, f'White pixels: {recon_nonzero}', \n",
    "                                   transform=axes[1, i].transAxes, verticalalignment='top',\n",
    "                                   bbox=dict(boxstyle='round', facecolor='lightcoral', alpha=0.8))\n",
    "                    \n",
    "                    # Calculate pixel similarity\n",
    "                    pixel_match = np.sum(original_images[i] == reconstructed_images[i])\n",
    "                    total_pixels = original_images[i].size\n",
    "                    similarity_pct = pixel_match / total_pixels * 100\n",
    "                    \n",
    "                    # Add similarity info\n",
    "                    axes[1, i].text(0.02, 0.02, f'Similarity: {similarity_pct:.1f}%', \n",
    "                                   transform=axes[1, i].transAxes, verticalalignment='bottom',\n",
    "                                   bbox=dict(boxstyle='round', \n",
    "                                            facecolor='lightgreen' if similarity_pct > 80 else \n",
    "                                                     'yellow' if similarity_pct > 50 else 'lightcoral', \n",
    "                                            alpha=0.8))\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "                \n",
    "                # Print detailed comparison statistics\n",
    "                print(f\"\\nğŸ“Š Detailed Comparison Statistics:\")\n",
    "                total_similarity = 0\n",
    "                \n",
    "                for i in range(num_samples):\n",
    "                    orig_img = original_images[i]\n",
    "                    recon_img = reconstructed_images[i]\n",
    "                    \n",
    "                    # Pixel-level comparison\n",
    "                    exact_match = np.sum(orig_img == recon_img)\n",
    "                    total_pixels = orig_img.size\n",
    "                    pixel_similarity = exact_match / total_pixels * 100\n",
    "                    total_similarity += pixel_similarity\n",
    "                    \n",
    "                    # Pattern comparison (non-zero pixels)\n",
    "                    orig_pattern = (orig_img > 0).astype(int)\n",
    "                    recon_pattern = (recon_img > 0).astype(int)\n",
    "                    pattern_match = np.sum(orig_pattern == recon_pattern)\n",
    "                    pattern_similarity = pattern_match / total_pixels * 100\n",
    "                    \n",
    "                    print(f\"\\n   Sample #{i+1}:\")\n",
    "                    print(f\"     Exact pixel match: {exact_match:,}/{total_pixels:,} ({pixel_similarity:.1f}%)\")\n",
    "                    print(f\"     Pattern similarity: {pattern_match:,}/{total_pixels:,} ({pattern_similarity:.1f}%)\")\n",
    "                    print(f\"     Original non-zero pixels: {np.count_nonzero(orig_img):,}\")\n",
    "                    print(f\"     Reconstructed non-zero pixels: {np.count_nonzero(recon_img):,}\")\n",
    "                \n",
    "                avg_similarity = total_similarity / num_samples\n",
    "                print(f\"\\nğŸ¯ Average pixel similarity: {avg_similarity:.1f}%\")\n",
    "                \n",
    "                # Overall assessment\n",
    "                if avg_similarity >= 90:\n",
    "                    print(f\"   ğŸ‰ Excellent! Images are nearly identical\")\n",
    "                elif avg_similarity >= 70:\n",
    "                    print(f\"   âœ… Good! Images are quite similar\")\n",
    "                elif avg_similarity >= 50:\n",
    "                    print(f\"   âš ï¸  Moderate similarity - some differences detected\")\n",
    "                else:\n",
    "                    print(f\"   âŒ Low similarity - significant differences detected!\")\n",
    "                    print(f\"   ğŸ” Recommend investigating image generation algorithm\")\n",
    "                \n",
    "            else:\n",
    "                print(f\"   âŒ Failed to load sample images for comparison\")\n",
    "        else:\n",
    "            print(f\"   âŒ No common .dat files found for visual comparison\")\n",
    "    else:\n",
    "        print(f\"   âŒ Required directories not found for visual comparison\")\n",
    "        print(f\"      Original: {original_dir} ({'âœ…' if os.path.exists(original_dir) else 'âŒ'})\")\n",
    "        print(f\"      Reconstructed: {reconstructed_dir} ({'âœ…' if os.path.exists(reconstructed_dir) else 'âŒ'})\")\n",
    "\n",
    "else:\n",
    "    print(\"âš ï¸  VISUAL COMPARISON SKIPPED\")\n",
    "    print(\"   Cannot compare - missing original or reconstructed data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Performance Benchmark Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final comprehensive summary\n",
    "print(\"ğŸ“‹ COMPREHENSIVE GENERATION & COMPARISON SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Generation performance summary\n",
    "if 'total_elapsed' in locals():\n",
    "    print(f\"\\nâš¡ GENERATION PERFORMANCE:\")\n",
    "    print(f\"   Total generation time: {total_elapsed:.1f} seconds ({total_elapsed/60:.1f} minutes)\")\n",
    "    if reconstructed_stats:\n",
    "        total_images = sum(stats['images'] for stats in reconstructed_stats.values() if stats)\n",
    "        if total_images > 0:\n",
    "            images_per_second = total_images / total_elapsed\n",
    "            print(f\"   Images generated: {total_images:,}\")\n",
    "            print(f\"   Generation speed: {images_per_second:.1f} images/second\")\n",
    "\n",
    "# Comparison summary\n",
    "if original_available and reconstructed_stats:\n",
    "    print(f\"\\nğŸ” COMPARISON SUMMARY:\")\n",
    "    print(f\"   Original data: âœ… Available\")\n",
    "    print(f\"   Reconstructed data: âœ… Available\")\n",
    "    if 'overall_ratio' in locals():\n",
    "        print(f\"   Image count ratio: {overall_ratio:.3f}\")\n",
    "        print(f\"   Quality assessment: {assessment.split(': ')[1] if ': ' in assessment else assessment}\")\n",
    "    if 'avg_similarity' in locals():\n",
    "        print(f\"   Visual similarity: {avg_similarity:.1f}%\")\n",
    "elif original_available:\n",
    "    print(f\"\\nğŸ” COMPARISON SUMMARY:\")\n",
    "    print(f\"   Original data: âœ… Available\")\n",
    "    print(f\"   Reconstructed data: âŒ Generation failed\")\n",
    "elif reconstructed_stats:\n",
    "    print(f\"\\nğŸ” COMPARISON SUMMARY:\")\n",
    "    print(f\"   Original data: âŒ Not available\")\n",
    "    print(f\"   Reconstructed data: âœ… Successfully generated\")\n",
    "    print(f\"   Note: Cannot compare without original data\")\n",
    "else:\n",
    "    print(f\"\\nâŒ GENERATION FAILED\")\n",
    "    print(f\"   Both original and reconstructed data unavailable\")\n",
    "\n",
    "# Next steps\n",
    "print(f\"\\nğŸ“š NEXT STEPS:\")\n",
    "\n",
    "if reconstructed_stats and sum(1 for stats in reconstructed_stats.values() if stats) == 3:\n",
    "    print(f\"   âœ… Image generation completed successfully\")\n",
    "    print(f\"   ğŸš€ Ready to proceed with model training\")\n",
    "    print(f\"   ğŸ“ Run: ipynb/2_model_training.ipynb\")\n",
    "    \n",
    "    if original_available and 'overall_ratio' in locals():\n",
    "        if overall_ratio < 0.8:\n",
    "            print(f\"\\n   âš ï¸  Data Quality Recommendations:\")\n",
    "            print(f\"      ğŸ“Š Investigate reconstruction algorithm differences\")\n",
    "            print(f\"      ğŸ” Review filtering logic in create_original_format.py\")\n",
    "            print(f\"      ğŸ“ˆ Consider adjusting parameters to match original better\")\n",
    "        elif 'avg_similarity' in locals() and avg_similarity < 70:\n",
    "            print(f\"\\n   âš ï¸  Visual Quality Recommendations:\")\n",
    "            print(f\"      ğŸ–¼ï¸  Investigate image generation algorithm\")\n",
    "            print(f\"      ğŸ¨ Check price normalization and pixel mapping\")\n",
    "            print(f\"      ğŸ“ Verify image dimensions and data structures\")\n",
    "        else:\n",
    "            print(f\"\\n   ğŸ‰ High quality reconstruction achieved!\")\n",
    "            print(f\"   âœ… Both quantity and visual similarity are excellent\")\n",
    "else:\n",
    "    print(f\"   âŒ Image generation incomplete or failed\")\n",
    "    print(f\"   ğŸ”§ Debug create_original_format.py execution\")\n",
    "    print(f\"   ğŸ“‹ Check error logs and data file availability\")\n",
    "\n",
    "# Save summary to file\n",
    "summary_data = {\n",
    "    'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'generation_time_seconds': total_elapsed if 'total_elapsed' in locals() else None,\n",
    "    'original_available': original_available,\n",
    "    'reconstructed_stats': reconstructed_stats,\n",
    "    'comparison_ratio': overall_ratio if 'overall_ratio' in locals() else None,\n",
    "    'visual_similarity': avg_similarity if 'avg_similarity' in locals() else None\n",
    "}\n",
    "\n",
    "import json\n",
    "os.makedirs('analysis_results', exist_ok=True)\n",
    "with open('analysis_results/image_generation_summary.json', 'w') as f:\n",
    "    json.dump(summary_data, f, indent=2, default=str)\n",
    "\n",
    "print(f\"\\nğŸ’¾ Summary saved to: analysis_results/image_generation_summary.json\")\n",
    "print(f\"\\nğŸ‰ Image generation and comparison analysis completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}