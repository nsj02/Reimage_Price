{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Data Preprocessing - Missing Value Treatment\n",
    "\n",
    "**Purpose**: Create two versions of dataset for performance comparison\n",
    "- **Original**: Keep missing values as-is (current version)\n",
    "- **Filled**: Fill missing open prices with previous day's close price\n",
    "\n",
    "**Missing Value Statistics**: ~1.8M missing open values out of 34.3M total records (5.3%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Data Preprocessing - Missing Value Treatment\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load original data\n",
    "print(\"Loading original datasets...\")\n",
    "\n",
    "train_file = 'data/data_1993_2000_train_val.parquet'\n",
    "test_file = 'data/data_2001_2019_test.parquet'\n",
    "\n",
    "train_df = pd.read_parquet(train_file)\n",
    "test_df = pd.read_parquet(test_file)\n",
    "\n",
    "print(f\"Train data: {len(train_df):,} records\")\n",
    "print(f\"Test data: {len(test_df):,} records\")\n",
    "print(f\"Total: {len(train_df) + len(test_df):,} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze missing values in original data\n",
    "def analyze_missing_values(df, dataset_name):\n",
    "    print(f\"\\n{dataset_name} Missing Value Analysis:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    key_columns = ['open', 'high', 'low', 'close', 'volume', 'ret']\n",
    "    \n",
    "    for col in key_columns:\n",
    "        if col in df.columns:\n",
    "            missing_count = df[col].isnull().sum()\n",
    "            missing_pct = missing_count / len(df) * 100\n",
    "            print(f\"  {col:8}: {missing_count:,} ({missing_pct:.2f}%)\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "analyze_missing_values(train_df, \"Training Data\")\n",
    "analyze_missing_values(test_df, \"Test Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing open prices with previous day's close price\n",
    "def fill_missing_open_prices(df, dataset_name):\n",
    "    print(f\"\\nProcessing {dataset_name}...\")\n",
    "    \n",
    "    df_filled = df.copy()\n",
    "    original_missing = df_filled['open'].isnull().sum()\n",
    "    \n",
    "    # Sort by stock code and date for proper forward fill\n",
    "    df_filled = df_filled.sort_values(['code', 'date']).reset_index(drop=True)\n",
    "    \n",
    "    # Fill missing open prices with previous day's close price\n",
    "    # Group by stock code to avoid cross-stock contamination\n",
    "    filled_count = 0\n",
    "    \n",
    "    for code in df_filled['code'].unique():\n",
    "        mask = df_filled['code'] == code\n",
    "        stock_data = df_filled[mask].copy()\n",
    "        \n",
    "        # Create previous day's close price column\n",
    "        stock_data['prev_close'] = stock_data['close'].shift(1)\n",
    "        \n",
    "        # Fill missing open with previous close\n",
    "        missing_open_mask = stock_data['open'].isnull()\n",
    "        stock_data.loc[missing_open_mask, 'open'] = stock_data.loc[missing_open_mask, 'prev_close']\n",
    "        \n",
    "        # Count filled values\n",
    "        filled_count += missing_open_mask.sum()\n",
    "        \n",
    "        # Update main dataframe\n",
    "        df_filled.loc[mask, 'open'] = stock_data['open']\n",
    "    \n",
    "    remaining_missing = df_filled['open'].isnull().sum()\n",
    "    actually_filled = original_missing - remaining_missing\n",
    "    \n",
    "    print(f\"  Original missing: {original_missing:,}\")\n",
    "    print(f\"  Filled: {actually_filled:,}\")\n",
    "    print(f\"  Still missing: {remaining_missing:,} (first day of each stock)\")\n",
    "    \n",
    "    return df_filled\n",
    "\n",
    "# Process both datasets\n",
    "train_filled = fill_missing_open_prices(train_df, \"Training Data\")\n",
    "test_filled = fill_missing_open_prices(test_df, \"Test Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare statistics before and after filling\n",
    "def compare_statistics(original_df, filled_df, dataset_name):\n",
    "    print(f\"\\n{dataset_name} - Before vs After Comparison:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    key_stats = ['count', 'mean', 'std', 'min', '25%', '50%', '75%', 'max']\n",
    "    \n",
    "    orig_stats = original_df['open'].describe()\n",
    "    filled_stats = filled_df['open'].describe()\n",
    "    \n",
    "    print(f\"{'Statistic':<10} {'Original':<12} {'Filled':<12} {'Difference':<12}\")\n",
    "    print(\"-\" * 48)\n",
    "    \n",
    "    for stat in key_stats:\n",
    "        if stat == 'count':\n",
    "            orig_val = orig_stats[stat]\n",
    "            filled_val = filled_stats[stat]\n",
    "            diff = filled_val - orig_val\n",
    "            print(f\"{stat:<10} {orig_val:<12.0f} {filled_val:<12.0f} {diff:<12.0f}\")\n",
    "        else:\n",
    "            orig_val = orig_stats[stat]\n",
    "            filled_val = filled_stats[stat]\n",
    "            diff = filled_val - orig_val\n",
    "            print(f\"{stat:<10} {orig_val:<12.4f} {filled_val:<12.4f} {diff:<12.4f}\")\n",
    "\n",
    "compare_statistics(train_df, train_filled, \"Training Data\")\n",
    "compare_statistics(test_df, test_filled, \"Test Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save filled datasets with new filenames\n",
    "print(\"\\nSaving filled datasets...\")\n",
    "\n",
    "train_filled_file = 'data/data_1993_2000_train_val_filled.parquet'\n",
    "test_filled_file = 'data/data_2001_2019_test_filled.parquet'\n",
    "\n",
    "# Save filled versions\n",
    "train_filled.to_parquet(train_filled_file, index=False)\n",
    "test_filled.to_parquet(test_filled_file, index=False)\n",
    "\n",
    "print(f\"âœ… Saved: {train_filled_file}\")\n",
    "print(f\"âœ… Saved: {test_filled_file}\")\n",
    "\n",
    "# File size comparison\n",
    "import os\n",
    "\n",
    "def get_file_size_mb(filepath):\n",
    "    return os.path.getsize(filepath) / (1024**2)\n",
    "\n",
    "print(\"\\nFile Size Comparison:\")\n",
    "print(f\"  Original train: {get_file_size_mb(train_file):.1f}MB\")\n",
    "print(f\"  Filled train:   {get_file_size_mb(train_filled_file):.1f}MB\")\n",
    "print(f\"  Original test:  {get_file_size_mb(test_file):.1f}MB\")\n",
    "print(f\"  Filled test:    {get_file_size_mb(test_filled_file):.1f}MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation: Check that filled data makes sense\n",
    "print(\"\\nValidation Checks:\")\n",
    "print(\"-\" * 20)\n",
    "\n",
    "# Check 1: No negative open prices after filling\n",
    "train_neg_open = (train_filled['open'] <= 0).sum()\n",
    "test_neg_open = (test_filled['open'] <= 0).sum()\n",
    "print(f\"âœ“ Non-positive open prices: Train={train_neg_open}, Test={test_neg_open}\")\n",
    "\n",
    "# Check 2: Reasonable open/close ratios\n",
    "train_filled['open_close_ratio'] = train_filled['open'] / train_filled['close']\n",
    "test_filled['open_close_ratio'] = test_filled['open'] / test_filled['close']\n",
    "\n",
    "extreme_ratios_train = ((train_filled['open_close_ratio'] > 2) | (train_filled['open_close_ratio'] < 0.5)).sum()\n",
    "extreme_ratios_test = ((test_filled['open_close_ratio'] > 2) | (test_filled['open_close_ratio'] < 0.5)).sum()\n",
    "\n",
    "print(f\"âœ“ Extreme open/close ratios (>2x or <0.5x): Train={extreme_ratios_train}, Test={extreme_ratios_test}\")\n",
    "\n",
    "# Check 3: Overall data integrity\n",
    "print(f\"âœ“ Data integrity: Train shape={train_filled.shape}, Test shape={test_filled.shape}\")\n",
    "\n",
    "print(\"\\nðŸŽ‰ Data preprocessing completed successfully!\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Generate images with both versions using datageneration.py\")\n",
    "print(\"2. Train models with both datasets\")\n",
    "print(\"3. Compare performance in data_analysis.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summary for next steps\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATASET VERSIONS READY FOR COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nðŸ“ Original Version (with missing values):\")\n",
    "print(\"   - data/data_1993_2000_train_val.parquet\")\n",
    "print(\"   - data/data_2001_2019_test.parquet\")\n",
    "print(\"   - Use: python datageneration.py --data_version original\")\n",
    "\n",
    "print(\"\\nðŸ“ Filled Version (missing values filled):\")\n",
    "print(\"   - data/data_1993_2000_train_val_filled.parquet\")\n",
    "print(\"   - data/data_2001_2019_test_filled.parquet\")\n",
    "print(\"   - Use: python datageneration.py --data_version filled\")\n",
    "\n",
    "print(\"\\nðŸ”¬ Performance Comparison Plan:\")\n",
    "print(\"   1. Generate 20d images for both versions\")\n",
    "print(\"   2. Train CNN20d models separately\")\n",
    "print(\"   3. Compare Sharpe ratios and portfolio performance\")\n",
    "print(\"   4. Analyze in data_analysis.ipynb\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Hypothesis:\")\n",
    "print(\"   Filled version may show improved performance due to:\")\n",
    "print(\"   - More complete candlestick patterns\")\n",
    "print(\"   - Reduced noise from missing data\")\n",
    "print(\"   - Better gradient flow during training\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}