#!/usr/bin/env python3
"""
chart_dataset.py - PyTorch Dataset for chart-based stock price prediction

Based on trend_submit/Data/chart_dataset.py
Loads .dat/.feather files generated by chart_generator.py
"""

import pandas as pd
import os
import os.path as op
import numpy as np
from PIL import Image
from torch.utils.data import Dataset


class EquityDataset(Dataset):
    """
    PyTorch Dataset for 2D chart-based stock price prediction.
    
    This dataset loads price chart images (candlestick/bar charts) and associated
    labels for training CNN models. Compatible with chart_generator.py output.
    """
    
    def __init__(
        self,
        window_size,           # Number of days in input window (5, 20, 60)
        predict_window,        # Number of days for return prediction (5, 20, 60)
        freq,                  # Data frequency ('week', 'month', 'quarter')
        year,                  # Year of data to load
        data_dir=".",          # Directory containing img_data_generated/
        has_volume_bar=True,   # Include volume bars in charts
        has_ma=True,           # Include moving average lines
        chart_type="bar",      # Chart type
        annual_stocks_num="all", # Number of stocks to include per year
        tstat_threshold=0,     # T-statistic threshold for filtering
        stockid_filter=None,   # Specific stock IDs to include
        remove_tail=False,     # Remove end-of-year data
        regression_label=None, # Label type (None=classification)
        delayed_ret=0,         # Delay in return calculation
    ):
        # Store configuration parameters
        self.ws = window_size
        self.pw = predict_window
        self.freq = freq
        assert self.freq in ["week", "month", "quarter"]
        self.year = year
        self.data_dir = data_dir
        self.has_vb = has_volume_bar
        self.has_ma = has_ma
        self.chart_type = chart_type
        self.regression_label = regression_label
        
        # Set image dimensions
        self.image_height = {5: 32, 20: 64, 60: 96}[window_size]
        self.image_width = {5: 15, 20: 60, 60: 180}[window_size]
        
        # Load image data and associated labels
        self.images, self.label_dict = self.load_images_and_labels()
        
        # Calculate normalization parameters
        self.demean = self._get_insample_mean_std()
        
        # Configure return label name
        self.ret_val_name = f"Ret_{predict_window}d"
        
        # Generate labels from return data
        self.label = self.get_label_value()
        
        # Apply data filtering
        self.filter_data(
            annual_stocks_num, stockid_filter, tstat_threshold, remove_tail
        )
    
    def get_image_label_save_path(self):
        """Get file paths for images and labels"""
        # Map window_size to subdirectory and prefix
        subdir_map = {5: "weekly_5d", 20: "monthly_20d", 60: "quarterly_60d"}
        prefix_map = {
            5: "5d_week_has_vb_[5]_ma",
            20: "20d_month_has_vb_[20]_ma", 
            60: "60d_quarter_has_vb_[60]_ma"
        }
        
        subdir = subdir_map[self.ws]
        prefix = prefix_map[self.ws]
        
        save_dir = op.join(self.data_dir, "img_data_generated", subdir)
        img_save_path = op.join(save_dir, f"{prefix}_{self.year}_images.dat")
        label_path = op.join(save_dir, f"{prefix}_{self.year}_labels_w_delay.feather")
        
        return img_save_path, label_path
    
    @staticmethod
    def load_image_np_data(img_save_path, image_height, image_width):
        """Load images from .dat binary file"""
        if not op.exists(img_save_path):
            raise FileNotFoundError(f"Image file not found: {img_save_path}")
            
        # Calculate number of images from file size
        file_size = op.getsize(img_save_path)
        pixels_per_image = image_height * image_width
        num_images = file_size // pixels_per_image
        
        # Load as memory-mapped array
        images = np.memmap(
            img_save_path, 
            dtype=np.uint8, 
            mode='r',
            shape=(num_images, image_height, image_width)
        )
        
        # Reshape to match trend_submit format: (N, 1, H, W)
        images = images.reshape((-1, 1, image_height, image_width))
        return images
    
    def load_images_and_labels(self):
        """Load images and labels for the specified year"""
        img_save_path, label_path = self.get_image_label_save_path()
        
        print(f"Loading images from {img_save_path}")
        images = self.load_image_np_data(img_save_path, self.image_height, self.image_width)
        
        print(f"Loading labels from {label_path}")
        if not op.exists(label_path):
            raise FileNotFoundError(f"Label file not found: {label_path}")
            
        label_df = pd.read_feather(label_path)
        label_df["StockID"] = label_df["StockID"].astype(str)
        
        # Convert to dict format like trend_submit
        label_dict = {c: np.array(label_df[c]) for c in label_df.columns}
        
        print(f"Loaded {len(images)} images and {len(label_df)} labels for year {self.year}")
        return images, label_dict
    
    def get_label_value(self):
        """Generate binary labels from return values"""
        print(f"Using {self.ret_val_name} as label")
        
        ret_vals = self.label_dict[self.ret_val_name]
        
        # Convert to binary labels: 1 if return > 0, 0 if return <= 0
        labels = np.where(ret_vals > 0, 1, 0)
        
        # Handle NaN values: set to -99 (will be filtered out)
        nan_mask = np.isnan(ret_vals)
        labels[nan_mask] = -99
        
        print(f"Label distribution: {np.bincount(labels[labels != -99])}")
        return labels
    
    def filter_data(self, annual_stocks_num, stockid_filter, tstat_threshold, remove_tail):
        """Apply various filtering criteria to the dataset"""
        
        # Create base filtering index
        idx = (
            pd.Series(self.label != -99)  # Valid labels (not NaN)
            & pd.Series(self.label_dict.get("EWMA_vol", np.ones(len(self.label))) != 0.0)  # Non-zero volatility
        )
        
        # Apply stock ID filtering if specified
        if stockid_filter is not None:
            stockid_idx = pd.Series(self.label_dict["StockID"]).isin(stockid_filter)
            idx = idx & stockid_idx
        
        # Apply t-statistic filtering
        if tstat_threshold > 0:
            tstats = np.divide(
                self.label_dict[self.ret_val_name], 
                np.sqrt(self.label_dict.get("EWMA_vol", np.ones(len(self.label))))
            )
            tstats = np.abs(tstats)
            t_th = np.nanpercentile(tstats[idx], tstat_threshold)
            tstat_idx = tstats > t_th
            print(f"Before filtering bottom {tstat_threshold}% tstats, sample size: {np.sum(idx)}")
            idx = idx & tstat_idx
            print(f"After filtering bottom {tstat_threshold}% tstats, sample size: {np.sum(idx)}")
        
        # Remove end-of-year data if specified
        if remove_tail:
            dates = pd.to_datetime(self.label_dict["Date"], format='%Y%m%d')
            last_day = "12/24" if self.pw == 5 else "12/1" if self.pw == 20 else "10/1"
            last_day = pd.Timestamp(f"{last_day}/{self.year}")
            idx = idx & (dates < last_day)
            print(f"After removing tail data: {np.sum(idx)}")
        
        # Apply filtering
        self.label = self.label[idx]
        print(f"Year {self.year}: final sample size: {len(self.label)}")
        
        for k in self.label_dict.keys():
            self.label_dict[k] = self.label_dict[k][idx]
        self.images = self.images[idx]
        self.label_dict["StockID"] = self.label_dict["StockID"].astype(str)
    
    def _get_insample_mean_std(self):
        """Calculate normalization parameters for images"""
        # Sample subset for efficiency
        sample_size = min(1000, len(self.images))
        sample_indices = np.random.choice(len(self.images), sample_size, replace=False)
        sample_images = self.images[sample_indices]
        
        # Convert to [0,1] range
        sample_images = sample_images.astype(np.float32) / 255.0
        
        mean = np.mean(sample_images)
        std = np.std(sample_images)
        
        print(f"Image normalization: mean={mean:.4f}, std={std:.4f}")
        return [mean, std]
    
    def __len__(self):
        return len(self.label)
    
    def __getitem__(self, idx):
        """Get a single sample from the dataset"""
        # Normalize image: scale to [0,1] then standardize
        image = (self.images[idx] / 255.0 - self.demean[0]) / self.demean[1]
        
        sample = {
            "image": image[0],  # Remove extra dimension: (1, H, W) -> (H, W)
            "label": self.label[idx],
            "ret_val": self.label_dict[self.ret_val_name][idx],
            "ending_date": self.label_dict["Date"][idx],
            "StockID": self.label_dict["StockID"][idx],
            "MarketCap": self.label_dict.get("MarketCap", np.zeros(len(self.label)))[idx],
        }
        return sample


if __name__ == "__main__":
    # Test the dataset
    data_dir = "/Users/nsj/내 드라이브/CNN_TRADING/ReImaging_Price_Trends/data"
    
    # Test with 20-day data for 1993
    dataset = EquityDataset(
        window_size=20,
        predict_window=20,
        freq="month",
        year=1993,
        data_dir=data_dir
    )
    
    print(f"Dataset size: {len(dataset)}")
    
    # Test sample
    if len(dataset) > 0:
        sample = dataset[0]
        print(f"\nSample structure:")
        for key, value in sample.items():
            if isinstance(value, np.ndarray):
                print(f"  {key}: {value.shape} {value.dtype}")
            else:
                print(f"  {key}: {value} ({type(value)})")