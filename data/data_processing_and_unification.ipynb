{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-Imaging Price Trends - Data Generation\n",
    "\n",
    "**Purpose**: Generate WRDS CRSP stock price data for paper reproduction\n",
    "\n",
    "**Output**: \n",
    "- `data_1993_2000_train_val.parquet` - Training/validation data\n",
    "- `data_2001_2019_test.parquet` - Test data\n",
    "\n",
    "**Next Step**: Run image generation scripts after completion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install wrds\n",
    "\n",
    "import wrds\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Environment setup completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. WRDS CRSP Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRDS Connection\n",
    "print(\"üîó Connecting to WRDS...\")\n",
    "db = wrds.Connection()\n",
    "print(\"‚úÖ Connected successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRSP Data Query - Full Dataset (1992-2019)\n",
    "query = \"\"\"\n",
    "    SELECT\n",
    "        a.permno AS code,\n",
    "        a.date,\n",
    "        a.prc AS close,\n",
    "        a.vol AS volume,\n",
    "        a.ret,          -- returns\n",
    "        a.openprc AS open,\n",
    "        a.askhi AS high,\n",
    "        a.bidlo AS low,\n",
    "        a.shrout,       -- shares outstanding (for market cap)\n",
    "        b.ticker\n",
    "    FROM\n",
    "        crsp.dsf AS a\n",
    "    LEFT JOIN\n",
    "        crsp.dsenames AS b\n",
    "        ON a.permno = b.permno AND b.namedt <= a.date AND a.date <= b.nameendt\n",
    "    WHERE\n",
    "        a.date BETWEEN '1992-01-01' AND '2019-12-31'\n",
    "        AND b.shrcd IN (10, 11)  -- Common stocks only\n",
    "        AND b.exchcd IN (1, 2, 3)  -- NYSE, AMEX, NASDAQ\n",
    "\"\"\"\n",
    "\n",
    "print(\"üì• Downloading CRSP data (1992-2019)... (This may take several minutes)\")\n",
    "df_raw = db.raw_sql(query, date_cols=['date'])\n",
    "db.close()\n",
    "\n",
    "print(f\"‚úÖ Download completed!\")\n",
    "print(f\"üìä Raw data: {len(df_raw):,} records\")\n",
    "print(f\"üè¢ Unique stocks: {df_raw['code'].nunique():,}\")\n",
    "print(f\"üìÖ Period: {df_raw['date'].min()} ~ {df_raw['date'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Data Analysis\n",
    "print(\"üìä RAW DATA ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"Total records: {len(df_raw):,}\")\n",
    "print(f\"Unique companies: {df_raw['code'].nunique():,}\")\n",
    "print(f\"Date range: {df_raw['date'].min()} to {df_raw['date'].max()}\")\n",
    "print(f\"Trading days: {df_raw['date'].nunique():,}\")\n",
    "\n",
    "# Missing values\n",
    "missing_stats = df_raw.isnull().sum()\n",
    "for col, missing_count in missing_stats.items():\n",
    "    if missing_count > 0:\n",
    "        pct = missing_count / len(df_raw) * 100\n",
    "        print(f\"Missing {col}: {missing_count:,} ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing (equity_data.py Î∞©Ïãù Ï†ÅÏö©)\n",
    "df = df_raw.copy()\n",
    "\n",
    "# 1. Ïª¨ÎüºÎ™Ö Î≥ÄÍ≤Ω (equity_data.py Î∞©Ïãù)\n",
    "df = df.rename(columns={\n",
    "    'code': 'StockID',      # WRDSÏùò code ‚Üí equity_dataÏùò StockID\n",
    "    'date': 'Date',         # ÎèôÏùº\n",
    "    'close': 'Close',       # ÎèôÏùº  \n",
    "    'volume': 'Vol',        # WRDSÏùò volume ‚Üí equity_dataÏùò Vol\n",
    "    'ret': 'Ret',          # ÎèôÏùº\n",
    "    'open': 'Open',        # ÎèôÏùº\n",
    "    'high': 'High',        # ÎèôÏùº\n",
    "    'low': 'Low',          # ÎèôÏùº\n",
    "    'shrout': 'Shares',    # WRDSÏùò shrout ‚Üí equity_dataÏùò Shares\n",
    "})\n",
    "\n",
    "# 2. Îç∞Ïù¥ÌÑ∞ ÌÉÄÏûÖ Î≥ÄÌôò (equity_data.py Î∞©Ïãù)\n",
    "df.StockID = df.StockID.astype(str)\n",
    "df.Ret = df.Ret.astype(str)\n",
    "\n",
    "# 3. Î¨¥Ìö®Ìïú Í∞íÎì§ NaNÏúºÎ°ú Ï≤òÎ¶¨ (equity_data.py Î∞©Ïãù)\n",
    "df = df.replace({\n",
    "    \"Close\": {0: np.nan},   \n",
    "    \"Open\": {0: np.nan},\n",
    "    \"High\": {0: np.nan},\n",
    "    \"Low\": {0: np.nan},\n",
    "    \"Ret\": {\"C\": np.nan, \"B\": np.nan, \"A\": np.nan, \".\": np.nan},\n",
    "    \"Vol\": {0: np.nan, (-99): np.nan},\n",
    "})\n",
    "\n",
    "# 4. ÏàòÏùµÎ•†ÏùÑ numericÏúºÎ°ú Î≥ÄÌôòÌïòÍ≥† NaN Ï†úÍ±∞\n",
    "df[\"Ret\"] = df.Ret.astype(np.float64)\n",
    "df = df.dropna(subset=[\"Ret\"])\n",
    "\n",
    "# 5. Ï†àÎåìÍ∞í Ï≤òÎ¶¨ (CRSP ÏùåÏàò Í∞ÄÍ≤© Ï≤òÎ¶¨)\n",
    "df[[\"Close\", \"Open\", \"High\", \"Low\", \"Vol\", \"Shares\"]] = df[\n",
    "    [\"Close\", \"Open\", \"High\", \"Low\", \"Vol\", \"Shares\"]\n",
    "].abs()\n",
    "\n",
    "print(\"Data preprocessing (equity_data.py style) completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Market Cap, MultiIndex, Log Returns (equity_data.py Î∞©Ïãù)\n",
    "print(\"\\nCalculating market cap and setting up data structure...\")\n",
    "\n",
    "# 1. ÏãúÏû•Ï∫° Í≥ÑÏÇ∞ (equity_data.py Î∞©Ïãù)\n",
    "df[\"MarketCap\"] = np.abs(df[\"Close\"] * df[\"Shares\"])\n",
    "\n",
    "# 2. MultiIndex ÏÑ§Ï†ï (equity_data.py Î∞©Ïãù)\n",
    "df.set_index([\"Date\", \"StockID\"], inplace=True)\n",
    "df.sort_index(inplace=True)\n",
    "\n",
    "# 3. Î°úÍ∑∏ ÏàòÏùµÎ•† Î∞è ÎàÑÏ†Å Î°úÍ∑∏ ÏàòÏùµÎ•† Í≥ÑÏÇ∞ (equity_data.py Î∞©Ïãù)\n",
    "df[\"log_ret\"] = np.log(1 + df.Ret)\n",
    "df[\"cum_log_ret\"] = df.groupby(\"StockID\")[\"log_ret\"].cumsum(skipna=True)\n",
    "\n",
    "# 4. EWMA Î≥ÄÎèôÏÑ± (equity_data.pyÏôÄ Ï†ïÌôïÌûà ÎèôÏùº)\n",
    "print(\"Calculating EWMA volatility...\")\n",
    "df[\"EWMA_vol\"] = df.groupby(\"StockID\")[\"Ret\"].transform(\n",
    "    lambda x: (x**2).ewm(alpha=0.05).mean().shift(periods=1)  # 1-day lag\n",
    ")\n",
    "\n",
    "print(\"Data structure setup completed (equity_data.py style)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Future Returns & Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Future Returns Calculation (equity_data.py Î∞©Ïãù - ÌïµÏã¨Îßå)\n",
    "print(\"Computing future returns using cumulative log returns...\")\n",
    "\n",
    "# equity_data.py Lines 214-218: Í≥†Ï†ï Í∏∞Í∞Ñ ÏàòÏùµÎ•†Îßå Í≥ÑÏÇ∞ (Ïù¥ÎØ∏ÏßÄ ÏÉùÏÑ±Ïö©)\n",
    "for i in [5, 20, 60, 65, 180, 250, 260]:\n",
    "    print(f\"Calculating {i}d return\")\n",
    "    df[f\"Ret_{i}d\"] = df.groupby(\"StockID\")[\"cum_log_ret\"].apply(\n",
    "        lambda x: np.exp(x.shift(-i) - x) - 1  # equity_data.pyÏôÄ Ï†ïÌôïÌûà ÎèôÏùºÌïú Í≥µÏãù\n",
    "    )\n",
    "\n",
    "# Î∞±Î∂ÑÏú®Î°ú Î≥ÄÌôò (dataset.py Ìò∏ÌôòÏÑ±)\n",
    "df['ret5'] = df['Ret_5d'] * 100   \n",
    "df['ret20'] = df['Ret_20d'] * 100\n",
    "df['ret60'] = df['Ret_60d'] * 100\n",
    "\n",
    "# Binary labels\n",
    "df['label_5'] = np.where(df['Ret_5d'] > 0, 1, 0)\n",
    "df['label_20'] = np.where(df['Ret_20d'] > 0, 1, 0) \n",
    "df['label_60'] = np.where(df['Ret_60d'] > 0, 1, 0)\n",
    "\n",
    "print(\"Future returns calculated (equity_data.py style)\")\n",
    "print(\"Note: Period-based returns (week/month/quarter) skipped - for portfolio rebalancing later\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Quality Check (MultiIndex Íµ¨Ï°∞Ïóê ÎßûÍ≤å ÏàòÏ†ï)\n",
    "print(\"\\nData Quality Summary:\")\n",
    "print(f\"Total records: {len(df):,}\")\n",
    "print(f\"Unique companies: {df.index.get_level_values('StockID').nunique():,}\")\n",
    "print(f\"Date range: {df.index.get_level_values('Date').min()} to {df.index.get_level_values('Date').max()}\")\n",
    "\n",
    "# Check future returns availability\n",
    "for period in [5, 20, 60]:\n",
    "    ret_col = f'Ret_{period}d'\n",
    "    valid_count = df[ret_col].notna().sum()\n",
    "    up_count = df[f'label_{period}'].sum()\n",
    "    up_pct = up_count / valid_count * 100 if valid_count > 0 else 0\n",
    "    print(f\"{period}-day returns: {valid_count:,} valid ({up_pct:.1f}% up)\")\n",
    "\n",
    "print(\"\\nData ready for image generation (equity_data.py style)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample of final data (MultiIndex Íµ¨Ï°∞)\n",
    "print(\"\\nSample of processed data:\")\n",
    "sample_cols = ['Close', 'Vol', 'Ret', 'MarketCap', 'EWMA_vol', \n",
    "               'Ret_5d', 'Ret_20d', 'Ret_60d', 'ret5', 'ret20', 'ret60', \n",
    "               'label_5', 'label_20', 'label_60']\n",
    "\n",
    "# MultiIndex Íµ¨Ï°∞ÏóêÏÑú ÏÉòÌîå Î≥¥Í∏∞\n",
    "print(\"First 10 records:\")\n",
    "print(df[sample_cols].head(10).to_string())\n",
    "\n",
    "print(\"\\nIndex structure:\")\n",
    "print(f\"Date index: {df.index.get_level_values('Date')[:5].tolist()}\")  \n",
    "print(f\"StockID index: {df.index.get_level_values('StockID')[:5].tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save Final Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save unified dataset (1992-2019 Ï†ÑÏ≤¥ ÌÜµÌï©)\n",
    "print(\"Saving unified dataset (1992-2019, equity_data.py processed)...\")\n",
    "\n",
    "# MultiIndexÎ•º resetÌï¥ÏÑú ÏùºÎ∞ò DataFrameÏúºÎ°ú Î≥ÄÌôò\n",
    "df_save = df.reset_index()\n",
    "\n",
    "# Ïª¨ÎüºÎ™ÖÏùÑ dataset.pyÍ∞Ä Í∏∞ÎåÄÌïòÎäî ÌòïÌÉúÎ°ú Î≥ÄÌôò\n",
    "df_save = df_save.rename(columns={\n",
    "    'StockID': 'code',        \n",
    "    'Date': 'date',           \n",
    "    'Close': 'close',         \n",
    "    'Vol': 'volume',          \n",
    "    'Ret': 'ret',            \n",
    "    'Open': 'open',          \n",
    "    'High': 'high',          \n",
    "    'Low': 'low',            \n",
    "    'MarketCap': 'mktcap',   \n",
    "    'EWMA_vol': 'ewma_vol'   \n",
    "})\n",
    "\n",
    "# ÌïòÎÇòÏùò ÌÜµÌï© ÌååÏùºÎ°ú Ï†ÄÏû• (Î∂ÑÌï† ÏóÜÏùå)\n",
    "unified_filename = 'data_1992_2019_unified.parquet'\n",
    "df_save.to_parquet(unified_filename, index=False)\n",
    "\n",
    "print(f\"‚úÖ Unified dataset: {unified_filename}\")\n",
    "print(f\"   Records: {len(df_save):,}\")\n",
    "print(f\"   Companies: {df_save['code'].nunique():,}\")\n",
    "print(f\"   Period: {df_save['date'].min()} ~ {df_save['date'].max()}\")\n",
    "\n",
    "print(f\"\\nüéâ Data generation completed!\")\n",
    "print(f\"   ‚Üí Single unified file with equity_data.py processing\")\n",
    "print(f\"   ‚Üí Ready for image generation with proper lookback data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary Statistics for Paper\n",
    "print(\"üìã SUMMARY STATISTICS FOR PAPER\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüîç Dataset Summary:\")\n",
    "print(f\"   ‚Ä¢ Period: 1992-2019 ({df_save['year'].nunique()} years)\")\n",
    "print(f\"   ‚Ä¢ Total observations: {len(df_save):,}\")\n",
    "print(f\"   ‚Ä¢ Unique companies: {df_save['code'].nunique():,}\")\n",
    "print(f\"   ‚Ä¢ Average company coverage: {len(df_save)/df_save['code'].nunique():.0f} obs/company\")\n",
    "\n",
    "print(f\"\\nüìä Label Distribution (Up/Down Movements):\")\n",
    "for period in [5, 20, 60]:\n",
    "    label_col = f'label_{period}'\n",
    "    valid_labels = df_save[label_col].dropna()\n",
    "    up_count = (valid_labels == 1).sum()\n",
    "    total_count = len(valid_labels)\n",
    "    up_pct = up_count / total_count * 100\n",
    "    \n",
    "    print(f\"   ‚Ä¢ {period}-day: {up_count:,}/{total_count:,} up ({up_pct:.1f}% up, {100-up_pct:.1f}% down)\")\n",
    "\n",
    "print(f\"\\nüí∞ Market Cap Distribution:\")\n",
    "valid_mktcap = df_save['mktcap'].dropna()\n",
    "percentiles = valid_mktcap.quantile([0.1, 0.25, 0.5, 0.75, 0.9])\n",
    "print(f\"   ‚Ä¢ 10th percentile: ${percentiles[0.1]/1e6:.0f}M\")\n",
    "print(f\"   ‚Ä¢ 25th percentile: ${percentiles[0.25]/1e6:.0f}M\") \n",
    "print(f\"   ‚Ä¢ Median: ${percentiles[0.5]/1e6:.0f}M\")\n",
    "print(f\"   ‚Ä¢ 75th percentile: ${percentiles[0.75]/1e6:.0f}M\")\n",
    "print(f\"   ‚Ä¢ 90th percentile: ${percentiles[0.9]/1e6:.0f}M\")\n",
    "\n",
    "print(f\"\\nüìà Return Characteristics:\")\n",
    "print(f\"   ‚Ä¢ Daily return volatility: {df_save['ret'].std()*100:.2f}% per day\")\n",
    "print(f\"   ‚Ä¢ 5-day return volatility: {df_save['ret5'].std():.2f}%\")\n",
    "print(f\"   ‚Ä¢ 20-day return volatility: {df_save['ret20'].std():.2f}%\")  \n",
    "print(f\"   ‚Ä¢ 60-day return volatility: {df_save['ret60'].std():.2f}%\")\n",
    "\n",
    "print(f\"\\n‚úÖ Data ready for CNN image generation and training!\")\n",
    "print(f\"   ‚Üí equity_data.py processing: ‚úÖ Complete\")\n",
    "print(f\"   ‚Üí Cumulative log return calculation: ‚úÖ Complete\") \n",
    "print(f\"   ‚Üí Future return labels: ‚úÖ Complete\")\n",
    "print(f\"   ‚Üí Missing value handling: ‚úÖ Complete\")\n",
    "print(f\"   ‚Üí 1992 lookback data: ‚úÖ Available for moving averages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Distribution Plots\n",
    "print(\"üìä CREATING DISTRIBUTION PLOTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Set up the plotting style\n",
    "plt.style.use('default')\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('Stock Data Distribution Analysis (1992-2019)', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Market Cap Distribution (Log Scale)\n",
    "valid_mktcap = df_save['mktcap'].dropna()\n",
    "axes[0,0].hist(np.log10(valid_mktcap), bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "axes[0,0].set_title('Market Cap Distribution (Log10)', fontweight='bold')\n",
    "axes[0,0].set_xlabel('Log10(Market Cap $)')\n",
    "axes[0,0].set_ylabel('Frequency')\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Daily Returns Distribution\n",
    "valid_ret = df_save['ret'].dropna()\n",
    "axes[0,1].hist(valid_ret * 100, bins=100, alpha=0.7, color='lightgreen', edgecolor='black', range=(-20, 20))\n",
    "axes[0,1].set_title('Daily Returns Distribution', fontweight='bold')  \n",
    "axes[0,1].set_xlabel('Daily Return (%)')\n",
    "axes[0,1].set_ylabel('Frequency')\n",
    "axes[0,1].axvline(x=0, color='red', linestyle='--', alpha=0.8)\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Price Level Distribution (Log Scale)\n",
    "valid_close = df_save['close'].dropna()\n",
    "axes[0,2].hist(np.log10(valid_close), bins=50, alpha=0.7, color='orange', edgecolor='black')\n",
    "axes[0,2].set_title('Stock Price Distribution (Log10)', fontweight='bold')\n",
    "axes[0,2].set_xlabel('Log10(Close Price $)')\n",
    "axes[0,2].set_ylabel('Frequency')\n",
    "axes[0,2].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. 5-day Future Returns\n",
    "valid_ret5 = df_save['ret5'].dropna()\n",
    "axes[1,0].hist(valid_ret5, bins=100, alpha=0.7, color='coral', edgecolor='black', range=(-50, 50))\n",
    "axes[1,0].set_title('5-Day Future Returns', fontweight='bold')\n",
    "axes[1,0].set_xlabel('5-Day Return (%)')\n",
    "axes[1,0].set_ylabel('Frequency')\n",
    "axes[1,0].axvline(x=0, color='red', linestyle='--', alpha=0.8)\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# 5. 20-day Future Returns  \n",
    "valid_ret20 = df_save['ret20'].dropna()\n",
    "axes[1,1].hist(valid_ret20, bins=100, alpha=0.7, color='mediumpurple', edgecolor='black', range=(-80, 80))\n",
    "axes[1,1].set_title('20-Day Future Returns', fontweight='bold')\n",
    "axes[1,1].set_xlabel('20-Day Return (%)')\n",
    "axes[1,1].set_ylabel('Frequency')\n",
    "axes[1,1].axvline(x=0, color='red', linestyle='--', alpha=0.8)\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "# 6. 60-day Future Returns\n",
    "valid_ret60 = df_save['ret60'].dropna()\n",
    "axes[1,2].hist(valid_ret60, bins=100, alpha=0.7, color='gold', edgecolor='black', range=(-150, 150))\n",
    "axes[1,2].set_title('60-Day Future Returns', fontweight='bold')\n",
    "axes[1,2].set_xlabel('60-Day Return (%)')\n",
    "axes[1,2].set_ylabel('Frequency')\n",
    "axes[1,2].axvline(x=0, color='red', linestyle='--', alpha=0.8)\n",
    "axes[1,2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('data_distribution_analysis.png', dpi=300, bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Distribution plots saved as: data_distribution_analysis.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time Series Analysis\n",
    "print(\"üìÖ TIME SERIES ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Returns by year\n",
    "print(f\"\\n1. Annual Statistics:\")\n",
    "df_save['year'] = pd.to_datetime(df_save['date']).dt.year\n",
    "\n",
    "for year in sorted(df_save['year'].unique()):\n",
    "    year_data = df_save[df_save['year'] == year]\n",
    "    \n",
    "    # Count data\n",
    "    total_obs = len(year_data)\n",
    "    total_companies = year_data['code'].nunique()\n",
    "    \n",
    "    # Daily returns\n",
    "    year_ret = year_data['ret'].dropna()\n",
    "    mean_daily_ret = year_ret.mean() * 100\n",
    "    \n",
    "    # 5d future returns  \n",
    "    year_ret5 = year_data['ret5'].dropna()\n",
    "    up_5d = (year_data['label_5'] == 1).sum()\n",
    "    total_5d = year_data['label_5'].notna().sum()\n",
    "    up_pct_5d = up_5d / total_5d * 100 if total_5d > 0 else 0\n",
    "    \n",
    "    print(f\"   {year}: {total_obs:,} obs, {total_companies:,} companies, \"\n",
    "          f\"daily_ret={mean_daily_ret:.3f}%, 5d_up={up_pct_5d:.1f}%\")\n",
    "\n",
    "# Market cap deciles analysis  \n",
    "print(f\"\\n2. Market Cap Decile Analysis (most recent year):\")\n",
    "recent_year = df_save['year'].max()\n",
    "recent_data = df_save[df_save['year'] == recent_year].copy()\n",
    "\n",
    "# Create market cap deciles\n",
    "recent_data = recent_data.dropna(subset=['mktcap'])\n",
    "if len(recent_data) > 0:\n",
    "    recent_data['mktcap_decile'] = pd.qcut(recent_data['mktcap'], \n",
    "                                          q=10, labels=False, duplicates='drop') + 1\n",
    "    \n",
    "    print(f\"   Based on {recent_year} data:\")\n",
    "    for decile in sorted(recent_data['mktcap_decile'].unique()):\n",
    "        decile_data = recent_data[recent_data['mktcap_decile'] == decile]\n",
    "        \n",
    "        # Market cap stats\n",
    "        mean_mktcap = decile_data['mktcap'].mean() / 1e6\n",
    "        median_mktcap = decile_data['mktcap'].median() / 1e6\n",
    "        \n",
    "        # Return stats\n",
    "        mean_ret = decile_data['ret'].mean() * 100\n",
    "        up_5d = (decile_data['label_5'] == 1).sum()\n",
    "        total_5d = decile_data['label_5'].notna().sum()\n",
    "        up_pct_5d = up_5d / total_5d * 100 if total_5d > 0 else 0\n",
    "        \n",
    "        print(f\"     Decile {decile:2.0f}: MktCap=${median_mktcap:6.0f}M, \"\n",
    "              f\"DailyRet={mean_ret:.3f}%, 5d_Up={up_pct_5d:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return Distribution Analysis (ÌïµÏã¨ Î∂ÑÏÑù)\n",
    "print(\"üìà RETURN DISTRIBUTION ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Daily returns\n",
    "print(f\"\\n1. Daily Returns (ret):\")\n",
    "valid_ret = df_save['ret'].dropna()\n",
    "positive_ret = (valid_ret > 0).sum()\n",
    "negative_ret = (valid_ret < 0).sum()\n",
    "zero_ret = (valid_ret == 0).sum()\n",
    "total_ret = len(valid_ret)\n",
    "\n",
    "print(f\"   Total observations: {total_ret:,}\")\n",
    "print(f\"   Positive returns: {positive_ret:,} ({positive_ret/total_ret*100:.1f}%)\")\n",
    "print(f\"   Negative returns: {negative_ret:,} ({negative_ret/total_ret*100:.1f}%)\")\n",
    "print(f\"   Zero returns: {zero_ret:,} ({zero_ret/total_ret*100:.1f}%)\")\n",
    "print(f\"   Mean: {valid_ret.mean()*100:.3f}% per day\")\n",
    "print(f\"   Std: {valid_ret.std()*100:.2f}% per day\")\n",
    "\n",
    "# Future returns analysis\n",
    "for period in [5, 20, 60]:\n",
    "    print(f\"\\n{period+1}. {period}-day Future Returns:\")\n",
    "    \n",
    "    # Percentage returns\n",
    "    ret_col = f'ret{period}'\n",
    "    label_col = f'label_{period}'\n",
    "    \n",
    "    valid_fut_ret = df_save[ret_col].dropna()\n",
    "    valid_labels = df_save[label_col].dropna()\n",
    "    \n",
    "    up_count = (valid_labels == 1).sum()\n",
    "    down_count = (valid_labels == 0).sum()\n",
    "    total_labels = len(valid_labels)\n",
    "    \n",
    "    print(f\"   Valid observations: {len(valid_fut_ret):,}\")\n",
    "    print(f\"   Up movements (>0%): {up_count:,} ({up_count/total_labels*100:.1f}%)\")\n",
    "    print(f\"   Down movements (‚â§0%): {down_count:,} ({down_count/total_labels*100:.1f}%)\")\n",
    "    print(f\"   Mean return: {valid_fut_ret.mean():.2f}%\")\n",
    "    print(f\"   Std return: {valid_fut_ret.std():.2f}%\")\n",
    "    print(f\"   Min return: {valid_fut_ret.min():.2f}%\")\n",
    "    print(f\"   Max return: {valid_fut_ret.max():.2f}%\")\n",
    "    \n",
    "    # Percentiles\n",
    "    percentiles = valid_fut_ret.quantile([0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.99])\n",
    "    print(f\"   Percentiles: 1%={percentiles[0.01]:.1f}%, 5%={percentiles[0.05]:.1f}%, 25%={percentiles[0.25]:.1f}%, 50%={percentiles[0.5]:.1f}%, 75%={percentiles[0.75]:.1f}%, 95%={percentiles[0.95]:.1f}%, 99%={percentiles[0.99]:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Data Distribution Analysis\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"üìä DATA DISTRIBUTION ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Basic statistics\n",
    "print(f\"\\n1. Dataset Overview:\")\n",
    "print(f\"   Total records: {len(df_save):,}\")\n",
    "print(f\"   Unique companies: {df_save['code'].nunique():,}\")\n",
    "print(f\"   Date range: {df_save['date'].min()} ~ {df_save['date'].max()}\")\n",
    "print(f\"   Trading days: {df_save['date'].nunique():,}\")\n",
    "\n",
    "# Market cap distribution\n",
    "print(f\"\\n2. Market Cap Distribution:\")\n",
    "valid_mktcap = df_save['mktcap'].dropna()\n",
    "print(f\"   Mean: ${valid_mktcap.mean()/1e6:.1f}M\")\n",
    "print(f\"   Median: ${valid_mktcap.median()/1e6:.1f}M\") \n",
    "print(f\"   Std: ${valid_mktcap.std()/1e6:.1f}M\")\n",
    "print(f\"   Min: ${valid_mktcap.min()/1e6:.1f}M\")\n",
    "print(f\"   Max: ${valid_mktcap.max()/1e6:.1f}M\")\n",
    "\n",
    "# Price distribution\n",
    "print(f\"\\n3. Price Level Distribution:\")\n",
    "valid_close = df_save['close'].dropna()\n",
    "print(f\"   Mean Close: ${valid_close.mean():.2f}\")\n",
    "print(f\"   Median Close: ${valid_close.median():.2f}\")\n",
    "print(f\"   Min Close: ${valid_close.min():.2f}\")\n",
    "print(f\"   Max Close: ${valid_close.max():.2f}\")\n",
    "\n",
    "# Volume distribution\n",
    "print(f\"\\n4. Volume Distribution:\")\n",
    "valid_volume = df_save['volume'].dropna()\n",
    "print(f\"   Mean Volume: {valid_volume.mean()/1e6:.1f}M shares\")\n",
    "print(f\"   Median Volume: {valid_volume.median()/1e6:.1f}M shares\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Exploratory Data Analysis (EDA)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
