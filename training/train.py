"""
train.py - CNN ëª¨ë¸ í›ˆë ¨ ë° ì„±ëŠ¥ ì‹œê°í™” ëª¨ë“ˆ

ì´ íŒŒì¼ì€ ìº”ë“¤ì°¨íŠ¸ ì´ë¯¸ì§€ë¡œ ì£¼ê°€ ì˜ˆì¸¡ ëª¨ë¸ì„ í›ˆë ¨ì‹œí‚¤ëŠ” í•µì‹¬ ê¸°ëŠ¥ë“¤ì„ í¬í•¨í•©ë‹ˆë‹¤:
- ëª¨ë¸ í›ˆë ¨ ë° ê²€ì¦ ë£¨í”„
- ì¡°ê¸° ì¢…ë£Œ (Early Stopping) ê¸°ëŠ¥
- í›ˆë ¨ ê³¼ì • ì‹œê°í™” (ì†ì‹¤/ì •í™•ë„ ê·¸ë˜í”„)
"""
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from utils.__init__ import *

# GPU ì‚¬ìš© ê°€ëŠ¥ ì‹œ CUDA, ì—†ìœ¼ë©´ CPU ì‚¬ìš©
device = 'cuda' if torch.cuda.is_available() else 'cpu'

def train_n_epochs(n_epochs, model, label_type, train_loader, valid_loader, criterion, optimizer, savefile, early_stop_epoch):
    """
    CNN ëª¨ë¸ì„ ì§€ì •ëœ ì—í¬í¬ ë™ì•ˆ í›ˆë ¨ì‹œí‚¤ëŠ” ë©”ì¸ í•¨ìˆ˜
    
    ì£¼ìš” ê¸°ëŠ¥:
    1. ğŸ‹ï¸ ë§¤ ì—í¬í¬ë§ˆë‹¤ í›ˆë ¨ ë°ì´í„°ë¡œ ëª¨ë¸ í•™ìŠµ
    2. ğŸ“Š ê²€ì¦ ë°ì´í„°ë¡œ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€  
    3. ğŸ’¾ ì„±ëŠ¥ì´ í–¥ìƒë  ë•Œë§ˆë‹¤ ìµœì  ëª¨ë¸ ì €ì¥
    4. â° ì„±ëŠ¥ í–¥ìƒì´ ë©ˆì¶”ë©´ ì¡°ê¸° ì¢…ë£Œ (ê³¼ì í•© ë°©ì§€)
    
    Args:
        n_epochs (int): ìµœëŒ€ í›ˆë ¨ ì—í¬í¬ ìˆ˜
        model: í›ˆë ¨í•  CNN ëª¨ë¸ (CNN5d, CNN20d, ë˜ëŠ” CNN60d)
        label_type (str): ì˜ˆì¸¡ ë¼ë²¨ íƒ€ì… ('RET5', 'RET20', ë˜ëŠ” 'RET60')
        train_loader: í›ˆë ¨ ë°ì´í„° ë¡œë”
        valid_loader: ê²€ì¦ ë°ì´í„° ë¡œë”  
        criterion: ì†ì‹¤ í•¨ìˆ˜ (BCELoss)
        optimizer: ìµœì í™” ì•Œê³ ë¦¬ì¦˜ (ì˜ˆ: Adam)
        savefile (str): ëª¨ë¸ ì €ì¥ ê²½ë¡œ
        early_stop_epoch (int): ì¡°ê¸° ì¢…ë£Œ ê¸°ì¤€ ì—í¬í¬ ìˆ˜
    
    Returns:
        tuple: (í›ˆë ¨ì†ì‹¤, ê²€ì¦ì†ì‹¤, í›ˆë ¨ì •í™•ë„, ê²€ì¦ì •í™•ë„) íˆìŠ¤í† ë¦¬
    """
    # === ì„±ëŠ¥ ì¶”ì  ë³€ìˆ˜ë“¤ ì´ˆê¸°í™” ===
    valid_loss_min = np.inf        # ìµœê³  ì„±ëŠ¥(ìµœì†Œ ê²€ì¦ ì†ì‹¤) ì¶”ì 
    train_loss_set = []            # ì—í¬í¬ë³„ í›ˆë ¨ ì†ì‹¤ ê¸°ë¡
    valid_loss_set = []            # ì—í¬í¬ë³„ ê²€ì¦ ì†ì‹¤ ê¸°ë¡
    train_acc_set = []             # ì—í¬í¬ë³„ í›ˆë ¨ ì •í™•ë„ ê¸°ë¡
    valid_acc_set = []             # ì—í¬í¬ë³„ ê²€ì¦ ì •í™•ë„ ê¸°ë¡
    invariant_epochs = 0           # ì„±ëŠ¥ í–¥ìƒì´ ì—†ëŠ” ì—í¬í¬ ì¹´ìš´í„° (ì¡°ê¸° ì¢…ë£Œìš©)
    
    # === ë©”ì¸ í›ˆë ¨ ë£¨í”„: ì§€ì •ëœ ì—í¬í¬ë§Œí¼ ë°˜ë³µ ===
    for epoch_i in range(1, n_epochs+1):
        
        # í˜„ì¬ ì—í¬í¬ì˜ ì†ì‹¤/ì •í™•ë„ ì´ˆê¸°í™”
        train_loss, train_acc = 0.0, 0.0
        valid_loss, valid_acc = 0.0, 0.0
        
        # ============== ğŸ‹ï¸ í›ˆë ¨ ë‹¨ê³„ (Training Phase) ==============
        model.train()  # ëª¨ë¸ì„ í›ˆë ¨ ëª¨ë“œë¡œ ì„¤ì • (Dropout, BatchNorm í™œì„±í™”)
        
        for i, batch in enumerate(train_loader):  # chart_dataset.py í˜•ì‹
            data = batch["image"]
            
            # ë¼ë²¨ íƒ€ì…ì— ë”°ë¼ ì˜ˆì¸¡ íƒ€ê²Ÿ ì„ íƒ (chart_datasetì€ ë‹¨ì¼ ë¼ë²¨ë§Œ ë°˜í™˜)
            target = batch["label"]  # ì´ë¯¸ ì„ íƒëœ prediction windowì˜ ë¼ë²¨
            
            # BCELossìš© ë¼ë²¨ (0~1 float, shape ë§ì¶¤)
            target = target.float().unsqueeze(1)  # [N] -> [N, 1]

            # GPUë¡œ ë°ì´í„° ì´ë™
            data, target = data.to(device), target.to(device)
            
            # ê²½ì‚¬ë„ ì´ˆê¸°í™” (ì´ì „ ë°°ì¹˜ì˜ ê²½ì‚¬ë„ ì œê±°)
            optimizer.zero_grad()
            
            # ìˆœì „íŒŒ: ëª¨ë¸ì— ì…ë ¥ì„ ë„£ì–´ ì˜ˆì¸¡ê°’ ê³„ì‚°
            output = model(data)
            
            # ì†ì‹¤ ê³„ì‚°
            loss = criterion(output, target)
            
            # ì—­ì „íŒŒ: ì†ì‹¤ì— ëŒ€í•œ ê²½ì‚¬ë„ ê³„ì‚°
            loss.backward()
            
            # ê²½ì‚¬ë„ë¥¼ ì´ìš©í•œ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸
            optimizer.step()
            
            # ì„±ëŠ¥ ì§€í‘œ ëˆ„ì 
            train_loss += loss.item() * data.size(0)  # ê°€ì¤‘ í‰ê· ì„ ìœ„í•´ ë°°ì¹˜ í¬ê¸° ê³±í•¨
            train_acc += ((output > 0.5).float() == target).sum()  # BCEëŠ” 0.5 ì„ê³„ê°’ìœ¼ë¡œ ë¶„ë¥˜

        # ============== ğŸ“Š ê²€ì¦ ë‹¨ê³„ (Validation Phase) ==============
        model.eval()  # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì • (Dropout ë¹„í™œì„±í™”, BatchNorm ê³ ì •)
        
        with torch.no_grad():  # ê²€ì¦ ë‹¨ê³„ì—ì„œëŠ” ê²½ì‚¬ë„ ê³„ì‚° ë¹„í™œì„±í™” (ë©”ëª¨ë¦¬ ì ˆì•½)
            for i, batch in enumerate(valid_loader):  # chart_dataset.py í˜•ì‹
                data = batch["image"]
                target = batch["label"]  # ì´ë¯¸ ì„ íƒëœ prediction windowì˜ ë¼ë²¨
                    
                # BCELossìš© ë¼ë²¨ (0~1 float, shape ë§ì¶¤)
                target = target.float().unsqueeze(1)  # [N] -> [N, 1]
                    
                # GPUë¡œ ë°ì´í„° ì´ë™
                data, target = data.to(device), target.to(device)
                
                # ìˆœì „íŒŒë§Œ ìˆ˜í–‰ (ì—­ì „íŒŒ ì—†ìŒ)
                output = model(data)
                loss = criterion(output, target)
                
                # ê²€ì¦ ì„±ëŠ¥ ì§€í‘œ ëˆ„ì 
                valid_loss += loss.item() * data.size(0)
                valid_acc += ((output > 0.5).float() == target).sum()  # BCEëŠ” 0.5 ì„ê³„ê°’ìœ¼ë¡œ ë¶„ë¥˜
        
        # ============== ğŸ“ˆ ì—í¬í¬ë³„ ì„±ëŠ¥ ê³„ì‚° ë° ê¸°ë¡ ==============
        
        # í‰ê·  ì†ì‹¤ ê³„ì‚° (ì „ì²´ ìƒ˜í”Œ ìˆ˜ë¡œ ë‚˜ëˆ„ê¸°)
        train_loss = train_loss / len(train_loader.dataset)
        train_loss_set.append(train_loss)
        valid_loss = valid_loss / len(valid_loader.dataset)
        valid_loss_set.append(valid_loss)

        # í‰ê·  ì •í™•ë„ ê³„ì‚° ë° ê¸°ë¡
        train_acc = train_acc / len(train_loader.dataset)
        train_acc_set.append(train_acc.detach().cpu().numpy())
        valid_acc = valid_acc / len(valid_loader.dataset)
        valid_acc_set.append(valid_acc.detach().cpu().numpy())
            
        # í˜„ì¬ ì—í¬í¬ ì„±ëŠ¥ ì¶œë ¥
        print('Epoch: {} Training Loss: {:.6f} Validation Loss: {:.6f} Training Acc: {:.5f} Validation Acc: {:.5f}'.format(
            epoch_i, train_loss, valid_loss, train_acc, valid_acc))
        
        # ============== ğŸ’¾ ëª¨ë¸ ì €ì¥ ë° ì¡°ê¸° ì¢…ë£Œ ë¡œì§ ==============
        
        # ê²€ì¦ ì†ì‹¤ì´ ê°œì„ ë˜ë©´ ëª¨ë¸ ì €ì¥
        if valid_loss <= valid_loss_min:
            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min, valid_loss))
            valid_loss_min = valid_loss
            invariant_epochs = 0  # ê°œì„ ë˜ì—ˆìœ¼ë¯€ë¡œ ì¹´ìš´í„° ë¦¬ì…‹
            
            # ëª¨ë¸, ì—í¬í¬, ì˜µí‹°ë§ˆì´ì € ìƒíƒœ ì €ì¥
            torch.save({
                'epoch': epoch_i,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict()
            }, savefile)
        else:
            # ì„±ëŠ¥ì´ ê°œì„ ë˜ì§€ ì•Šìœ¼ë©´ ì¹´ìš´í„° ì¦ê°€
            invariant_epochs += 1
        
        # ì¡°ê¸° ì¢…ë£Œ ì¡°ê±´ ì²´í¬
        if invariant_epochs >= early_stop_epoch:
            print(f"Early Stop at Epoch [{epoch_i}]: Performance hasn't enhanced for {early_stop_epoch} epochs")
            break

    # í›ˆë ¨ ê³¼ì •ì˜ ëª¨ë“  ì„±ëŠ¥ ê¸°ë¡ ë°˜í™˜
    return train_loss_set, valid_loss_set, train_acc_set, valid_acc_set



def plot_loss_and_acc(loss_and_acc_dict):
    """
    í›ˆë ¨ ê³¼ì •ì˜ ì†ì‹¤ ë° ì •í™•ë„ ë³€í™”ë¥¼ ì‹œê°í™”í•˜ëŠ” í•¨ìˆ˜
    
    ğŸ“Š ë‘ ê°œì˜ ê·¸ë˜í”„ë¥¼ ë‚˜ë€íˆ ìƒì„±:
    - ì™¼ìª½: ì—í¬í¬ë³„ ì†ì‹¤(Loss) ë³€í™” ì¶”ì´
    - ì˜¤ë¥¸ìª½: ì—í¬í¬ë³„ ì •í™•ë„(Accuracy) ë³€í™” ì¶”ì´
    
    Args:
        loss_and_acc_dict (dict): ê° ë°ì´í„°ì…‹ë³„ [ì†ì‹¤ë¦¬ìŠ¤íŠ¸, ì •í™•ë„ë¦¬ìŠ¤íŠ¸] ë”•ì…”ë„ˆë¦¬
                                  ì˜ˆ: {'train': ([ì†ì‹¤ë“¤], [ì •í™•ë„ë“¤]), 'valid': ([ì†ì‹¤ë“¤], [ì •í™•ë„ë“¤])}
    
    ì£¼ìš” ê¸°ëŠ¥:
    - í›ˆë ¨/ê²€ì¦ ì„±ëŠ¥ì„ í•œëˆˆì— ë¹„êµ ê°€ëŠ¥
    - ê³¼ì í•© ì—¬ë¶€ íŒë‹¨ (ê²€ì¦ ì†ì‹¤ì´ í›ˆë ¨ ì†ì‹¤ë³´ë‹¤ ë†’ì•„ì§€ëŠ” ì§€ì )
    - í•™ìŠµ ìˆ˜ë ´ ìƒíƒœ í™•ì¸
    """
    # 2ê°œ ì„œë¸Œí”Œë¡¯ ìƒì„± (1í–‰ 2ì—´, ê°€ë¡œë¡œ ë°°ì¹˜)
    _, axes = plt.subplots(1, 2, figsize=(20, 6))
    
    # ì²« ë²ˆì§¸ ë°ì´í„°ì…‹ì˜ ì†ì‹¤ ë¦¬ìŠ¤íŠ¸ ê¸¸ì´ë¡œ ì´ ì—í¬í¬ ìˆ˜ ê³„ì‚°
    tmp = list(loss_and_acc_dict.values())
    maxEpoch = len(tmp[0][0])

    # ========== ì™¼ìª½ ê·¸ë˜í”„: ì†ì‹¤(Loss) ì‹œê°í™” ==========
    
    # Yì¶• ë²”ìœ„ ìë™ ì„¤ì • (ëª¨ë“  ë°ì´í„°ì…‹ì˜ ìµœëŒ€/ìµœì†Œ ì†ì‹¤ ê¸°ì¤€)
    maxLoss = max([max(x[0]) for x in loss_and_acc_dict.values()]) + 0.1
    minLoss = max(0, min([min(x[0]) for x in loss_and_acc_dict.values()]) - 0.1)

    # ê° ë°ì´í„°ì…‹(í›ˆë ¨/ê²€ì¦)ë³„ë¡œ ì†ì‹¤ ê·¸ë˜í”„ ê·¸ë¦¬ê¸°
    for name, lossAndAcc in loss_and_acc_dict.items():
        axes[0].plot(range(1, 1 + maxEpoch), lossAndAcc[0], '-s', label=name)

    # ì†ì‹¤ ê·¸ë˜í”„ ì„¤ì •
    axes[0].set_xlabel('Epoch')
    axes[0].set_ylabel('Loss')
    axes[0].set_xticks(range(0, maxEpoch + 1, max(1, maxEpoch//10)))  # Xì¶• ëˆˆê¸ˆ (ìµœì†Œ ê°„ê²© 1)
    axes[0].axis([0, maxEpoch, minLoss, maxLoss])
    axes[0].legend()
    axes[0].set_title("Training & Validation Loss")

    # ========== ì˜¤ë¥¸ìª½ ê·¸ë˜í”„: ì •í™•ë„(Accuracy) ì‹œê°í™” ==========
    
    # Yì¶• ë²”ìœ„ ìë™ ì„¤ì • (ì •í™•ë„ëŠ” 0~1 ì‚¬ì´ì´ë¯€ë¡œ ë²”ìœ„ ì œí•œ)
    maxAcc = min(1, max([max(x[1]) for x in loss_and_acc_dict.values()]) + 0.1)
    minAcc = max(0, min([min(x[1]) for x in loss_and_acc_dict.values()]) - 0.1)

    # ê° ë°ì´í„°ì…‹ë³„ë¡œ ì •í™•ë„ ê·¸ë˜í”„ ê·¸ë¦¬ê¸°  
    for name, lossAndAcc in loss_and_acc_dict.items():
        axes[1].plot(range(1, 1 + maxEpoch), lossAndAcc[1], '-s', label=name)

    # ì •í™•ë„ ê·¸ë˜í”„ ì„¤ì •
    axes[1].set_xlabel('Epoch')
    axes[1].set_ylabel('Accuracy')
    axes[1].set_xticks(range(0, maxEpoch + 1, max(1, maxEpoch//10)))  # Xì¶• ëˆˆê¸ˆ
    axes[1].axis([0, maxEpoch, minAcc, maxAcc])
    axes[1].legend()
    axes[1].set_title("Training & Validation Accuracy")
    
    # ê·¸ë˜í”„ í‘œì‹œ
    plt.tight_layout()  # ì„œë¸Œí”Œë¡¯ ê°„ê²© ìë™ ì¡°ì •
    plt.show()


def main():
    """
    Main training function with argparse support
    """
    import argparse
    
    parser = argparse.ArgumentParser(description='Train CNN models for stock price prediction')
    parser.add_argument('--model', type=str, required=True, choices=['CNN5d', 'CNN20d', 'CNN60d'], 
                       help='Model architecture')
    parser.add_argument('--image_days', type=int, required=True, choices=[5, 20, 60],
                       help='Image window size in days')
    parser.add_argument('--pred_days', type=int, required=True, choices=[5, 20, 60],
                       help='Prediction horizon in days')
    parser.add_argument('--batch_size', type=int, default=128, help='Batch size')
    parser.add_argument('--learning_rate', type=float, default=1e-5, help='Learning rate')
    parser.add_argument('--epochs', type=int, default=50, help='Maximum epochs')
    parser.add_argument('--early_stop', type=int, default=2, help='Early stopping patience')
    parser.add_argument('--ensemble', action='store_true', help='Train ensemble models')
    parser.add_argument('--ensemble_runs', type=int, default=5, help='Number of ensemble models')
    parser.add_argument('--use_original_format', action='store_true', help='Use original .dat format')
    parser.add_argument('--data_version', type=str, default='reconstructed', 
                       choices=['original_author', 'reconstructed', 'filled'],
                       help='Data version: original_author (img_data), reconstructed (img_data_reconstructed), filled (img_data_reconstructed_filled)')
    
    args = parser.parse_args()
    
    # Model mapping
    from models import model
    model_classes = {
        'CNN5d': model.CNN5d,
        'CNN20d': model.CNN20d, 
        'CNN60d': model.CNN60d
    }
    
    # Label mapping
    label_map = {5: 'RET5', 20: 'RET20', 60: 'RET60'}
    label_type = label_map[args.pred_days]
    
    model_class = model_classes[args.model]
    
    # Model naming with data version distinction
    if args.data_version == 'original_author':
        version_suffix = "_original_author"
    elif args.data_version == 'filled':
        version_suffix = "_filled" 
    else:  # 'reconstructed'
        version_suffix = "_reconstructed"
    
    base_model_name = f"{args.model}_I{args.image_days}R{args.pred_days}{version_suffix}"
    
    if args.ensemble:
        print(f"Training ensemble models: {args.ensemble_runs} runs")
        for run_idx in range(1, args.ensemble_runs + 1):
            model_name = f"{base_model_name}_run{run_idx}"
            model_file = f"models/{model_name}.tar"
            
            if os.path.exists(model_file):
                print(f"Model {model_name} already exists - skipping")
                continue
                
            print(f"\nTraining ensemble model {run_idx}/{args.ensemble_runs}")
            train_single_model(args, model_class, label_type, model_name)
    else:
        model_file = f"models/{base_model_name}.tar"
        if os.path.exists(model_file):
            print(f"Model {base_model_name} already exists - skipping")
            return
            
        print(f"Training single model: {base_model_name}")
        train_single_model(args, model_class, label_type, base_model_name)


def train_single_model(args, model_class, label_type, model_name):
    """
    Train a single model
    """
    # Create models directory
    os.makedirs('models', exist_ok=True)
    
    print(f"Loading dataset: {args.data_version} data, {args.image_days}d window")
    
    # Load data
    if args.use_original_format:
        from data import chart_dataset
        import torch.utils.data
        
        # Load multiple years for training (1993-2000)
        datasets = []
        for year in range(1993, 2001):
            try:
                yearly_dataset = chart_dataset.EquityDataset(
                    window_size=args.image_days,
                    predict_window=args.pred_days,
                    freq="month",
                    year=year,
                    data_dir="../data"
                )
                datasets.append(yearly_dataset)
                print(f"Loaded year {year}: {len(yearly_dataset)} samples")
            except FileNotFoundError:
                print(f"Warning: No data found for year {year}")
                continue
        
        if len(datasets) == 0:
            print("Failed to load any dataset")
            return
            
        # Combine all years
        full_dataset = torch.utils.data.ConcatDataset(datasets)
        print(f"Total dataset size: {len(full_dataset)} samples")
        
        # Split train/val (70/30)
        train_size = int(0.7 * len(full_dataset))
        val_size = len(full_dataset) - train_size
        train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size])
        
    else:
        import dataset as dataset_module
        dataset_obj = dataset_module.ImageDataSet(args.image_days, 'train', label_type, data_version=args.data_version)
        images = dataset_obj.generate_images(sample_rate=1.0)
        
        # Split train/val (70/30)
        import numpy as np
        np.random.seed(42)
        shuffled_indices = np.random.permutation(len(images))
        split_idx = int(0.7 * len(images))
        train_indices = shuffled_indices[:split_idx]
        val_indices = shuffled_indices[split_idx:]
        train_images = [images[i] for i in train_indices]
        val_images = [images[i] for i in val_indices]
        
        train_dataset = torch.utils.data.TensorDataset(
            torch.stack([torch.from_numpy(img[0]).float() for img in train_images]),
            torch.tensor([img[1] for img in train_images], dtype=torch.long),
            torch.tensor([img[2] for img in train_images], dtype=torch.long),
            torch.tensor([img[3] for img in train_images], dtype=torch.long),
            torch.tensor([img[4] for img in train_images], dtype=torch.float),
            torch.tensor([img[5] for img in train_images], dtype=torch.float),
            torch.tensor([img[6] for img in train_images], dtype=torch.float)
        )
        val_dataset = torch.utils.data.TensorDataset(
            torch.stack([torch.from_numpy(img[0]).float() for img in val_images]),
            torch.tensor([img[1] for img in val_images], dtype=torch.long),
            torch.tensor([img[2] for img in val_images], dtype=torch.long),
            torch.tensor([img[3] for img in val_images], dtype=torch.long),
            torch.tensor([img[4] for img in val_images], dtype=torch.float),
            torch.tensor([img[5] for img in val_images], dtype=torch.float),
            torch.tensor([img[6] for img in val_images], dtype=torch.float)
        )
    
    # Create data loaders
    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)
    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False)
    
    print(f"Train samples: {len(train_dataset):,}")
    print(f"Val samples: {len(val_dataset):,}")
    
    # Create model
    model = model_class().to(device)
    criterion = torch.nn.BCELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)
    
    # Train model
    savefile = f"models/{model_name}.tar"
    train_loss, val_loss, train_acc, val_acc = train_n_epochs(
        args.epochs, model, label_type, train_loader, val_loader, 
        criterion, optimizer, savefile, args.early_stop
    )
    
    print(f"Training completed: {model_name}")
    
    # Plot results
    loss_and_acc_dict = {
        'train': (train_loss, train_acc),
        'validation': (val_loss, val_acc)
    }
    plot_loss_and_acc(loss_and_acc_dict)


if __name__ == '__main__':
    main()