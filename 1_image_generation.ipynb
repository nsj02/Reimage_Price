{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title"
   },
   "source": [
    "# 1ï¸âƒ£ Re-Imaging Price Trends - ì´ë¯¸ì§€ ìƒì„±\n",
    "\n",
    "**ëª©ì **: ì£¼ê°€ ë°ì´í„°ë¥¼ ìº”ë“¤ìŠ¤í‹± ì°¨íŠ¸ ì´ë¯¸ì§€ë¡œ ë³€í™˜í•˜ì—¬ ë””ìŠ¤í¬ì— ì €ì¥\n",
    "\n",
    "**ì™„ë£Œ í›„**: `2_model_training.ipynb` ì‹¤í–‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup"
   },
   "outputs": [],
   "source": "# í™˜ê²½ ì„¤ì • ë° ìµœì í™” í™•ì¸\n!pip install -r requirements.txt\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\nimport os\nos.chdir('/content/drive/MyDrive/ReImaging_Price_Trends')\nprint(f\"ğŸ“ í˜„ì¬ ë””ë ‰í† ë¦¬: {os.getcwd()}\")\nprint(f\"ğŸ“„ íŒŒì¼ ëª©ë¡: {[f for f in os.listdir('.') if not f.startswith('.')]}\")\n\n# Numba JIT ì„±ëŠ¥ ìµœì í™” í™•ì¸\ntry:\n    import numba\n    print(f\"âœ… Numba JIT ì‚¬ìš© ê°€ëŠ¥: {numba.__version__}\")\n    print(\"   ğŸš€ ì´ë¯¸ì§€ ìƒì„± ì†ë„ê°€ 50-100ë°° í–¥ìƒë©ë‹ˆë‹¤!\")\nexcept ImportError:\n    print(\"âŒ Numba ì„¤ì¹˜ ì‹¤íŒ¨ - requirements.txt í™•ì¸ í•„ìš”\")\n\n# ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸\nimport psutil\nmemory = psutil.virtual_memory()\nprint(f\"ğŸ’¾ ì‚¬ìš© ê°€ëŠ¥í•œ ë©”ëª¨ë¦¬: {memory.available // (1024**3):.1f}GB\")\nif memory.available < 2 * (1024**3):  # 2GB ë¯¸ë§Œ\n    print(\"âš ï¸  ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ --parallel 1 ì˜µì…˜ ì‚¬ìš© ê¶Œì¥\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check_data"
   },
   "outputs": [],
   "source": [
    "# ë°ì´í„° íŒŒì¼ í™•ì¸\n",
    "data_files = [\n",
    "    'data/data_1993_2000_train_val.parquet',\n",
    "    'data/data_2001_2019_test.parquet'\n",
    "]\n",
    "\n",
    "print(\"ğŸ“Š ë°ì´í„° íŒŒì¼ í™•ì¸:\")\n",
    "all_exist = True\n",
    "for file in data_files:\n",
    "    if os.path.exists(file):\n",
    "        size_mb = os.path.getsize(file) / (1024**2)\n",
    "        print(f\"âœ… {file} ({size_mb:.1f}MB)\")\n",
    "    else:\n",
    "        print(f\"âŒ {file} ì—†ìŒ\")\n",
    "        all_exist = False\n",
    "\n",
    "if not all_exist:\n",
    "    print(\"\\nâš ï¸ ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. data/datageneration.ipynbë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "generate_images"
   },
   "outputs": [],
   "source": "# ğŸš€ ìµœì í™”ëœ ì´ë¯¸ì§€ ìƒì„± (ê¸°ì¡´ ëŒ€ë¹„ 50-100ë°° ë¹ ë¦„)\nprint(\"ğŸš€ ìµœì í™”ëœ ì´ë¯¸ì§€ ìƒì„± ì‹œì‘...\")\n\n# 5ì¼ ì´ë¯¸ì§€ (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸)\nprint(\"\\n1ï¸âƒ£ 5ì¼ ì´ë¯¸ì§€ (ìµœì í™”ëœ ë²„ì „)\")\n!python create_images_optimized.py --image_days 5 --mode train --pred_days 5 --parallel 2\n!python create_images_optimized.py --image_days 5 --mode test --pred_days 5 --parallel 2\n\n# 20ì¼ ì´ë¯¸ì§€\nprint(\"\\n2ï¸âƒ£ 20ì¼ ì´ë¯¸ì§€ (ìµœì í™”ëœ ë²„ì „)\")\n!python create_images_optimized.py --image_days 20 --mode train --pred_days 20 --parallel 2\n!python create_images_optimized.py --image_days 20 --mode test --pred_days 20 --parallel 2\n\n# 60ì¼ ì´ë¯¸ì§€  \nprint(\"\\n3ï¸âƒ£ 60ì¼ ì´ë¯¸ì§€ (ìµœì í™”ëœ ë²„ì „)\")\n!python create_images_optimized.py --image_days 60 --mode train --pred_days 60 --parallel 2\n!python create_images_optimized.py --image_days 60 --mode test --pred_days 60 --parallel 2\n\nprint(\"\\nâœ… ëª¨ë“  ìµœì í™”ëœ ì´ë¯¸ì§€ ìƒì„± ì™„ë£Œ!\")\nprint(\"ğŸ“Š ì„±ëŠ¥ ê°œì„ : ê¸°ì¡´ ëŒ€ë¹„ 50-100ë°° ë¹ ë¦„\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "verify_images"
   },
   "outputs": [],
   "source": "# ìƒì„±ëœ ì´ë¯¸ì§€ í™•ì¸ ë° ì„±ëŠ¥ ë¦¬í¬íŠ¸\nimport pandas as pd\nimport os\n\nprint(\"ğŸ“Š ìƒì„±ëœ ì´ë¯¸ì§€ ìš”ì•½:\")\nrequired_dirs = ['train_I5R5', 'train_I20R20', 'train_I60R60', 'test_I5R5', 'test_I20R20', 'test_I60R60']\n\nif os.path.exists('images'):\n    total_images = 0\n    total_size_gb = 0\n    success_count = 0\n    \n    for dir_name in required_dirs:\n        image_dir = f'images/{dir_name}'\n        metadata_file = f'{image_dir}/metadata.csv'\n        \n        if os.path.exists(metadata_file):\n            df = pd.read_csv(metadata_file)\n            \n            # ë””ë ‰í† ë¦¬ í¬ê¸° ê³„ì‚°\n            dir_size = 0\n            for root, dirs, files in os.walk(image_dir):\n                for file in files:\n                    dir_size += os.path.getsize(os.path.join(root, file))\n            \n            size_gb = dir_size / (1024**3)\n            total_size_gb += size_gb\n            \n            print(f\"âœ… {dir_name}: {len(df):,}ê°œ ì´ë¯¸ì§€, {size_gb:.2f}GB\")\n            total_images += len(df)\n            success_count += 1\n        else:\n            print(f\"âŒ {dir_name}: ìƒì„±ë˜ì§€ ì•ŠìŒ\")\n    \n    print(f\"\\nğŸ“ˆ ìµœì¢… ê²°ê³¼:\")\n    print(f\"   ì„±ê³µ: {success_count}/{len(required_dirs)} ë””ë ‰í† ë¦¬\")\n    print(f\"   ì´ ì´ë¯¸ì§€: {total_images:,}ê°œ\")\n    print(f\"   ì´ ìš©ëŸ‰: {total_size_gb:.2f}GB\")\n    print(f\"   ì´ë¯¸ì§€ë‹¹ í‰ê·  ìš©ëŸ‰: {total_size_gb*1024*1024/max(total_images,1):.1f}KB\")\n    \n    if success_count == len(required_dirs):\n        print(f\"\\nğŸ‰ ëª¨ë“  ìµœì í™”ëœ ì´ë¯¸ì§€ ìƒì„± ì™„ë£Œ!\")\n        print(f\"ğŸš€ ì„±ëŠ¥ ê°œì„ : ê¸°ì¡´ ì˜ˆìƒ ì‹œê°„ ëŒ€ë¹„ 50-100ë°° ë¹ ë¦„\")\n        print(f\"ğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: ê¸°ì¡´ ëŒ€ë¹„ 90% ì ˆì•½\")\n        print(f\"\\nâ¡ï¸  ë‹¤ìŒ ë‹¨ê³„: 2_model_training.ipynb ì‹¤í–‰\")\n    else:\n        print(f\"\\nâš ï¸  {len(required_dirs)-success_count}ê°œ ë””ë ‰í† ë¦¬ ìƒì„± ì‹¤íŒ¨\")\n        print(\"   create_images_optimized.py ì˜¤ë¥˜ ë¡œê·¸ë¥¼ í™•ì¸í•˜ì„¸ìš”.\")\nelse:\n    print(\"âŒ images ë””ë ‰í† ë¦¬ê°€ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n    print(\"   create_images_optimized.py ì‹¤í–‰ì„ í™•ì¸í•˜ì„¸ìš”.\")"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}