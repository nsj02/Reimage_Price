{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Dataset Up/Down Distribution Analysis\n\nAnalyze the number of up/down stocks after 20 days in the entire dataset."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 한글 폰트 설정\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "plt.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Training Data Analysis (1993-2000)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load training data\nprint(\"=== Training Data (1993-2000) ===\")\ntrain_data = pd.read_parquet('data/data_1993_2000_train_val.parquet')\n\nprint(f\"Total data size: {len(train_data):,}\")\nprint(f\"Columns: {list(train_data.columns)}\")\nprint(f\"Date range: {train_data['date'].min()} ~ {train_data['date'].max()}\")\nprint(f\"Unique stocks: {train_data['code'].nunique():,}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 20-day up/down distribution (label_20)\nif 'label_20' in train_data.columns:\n    label_20_counts = train_data['label_20'].value_counts().sort_index()\n    print(f\"\\n20-day up/down distribution (training data):\")\n    print(f\"  Down (0): {label_20_counts.get(0, 0):,} ({label_20_counts.get(0, 0)/len(train_data)*100:.1f}%)\")\n    print(f\"  Up (1): {label_20_counts.get(1, 0):,} ({label_20_counts.get(1, 0)/len(train_data)*100:.1f}%)\")\n    \n    # Check NA values\n    na_count = train_data['label_20'].isna().sum()\n    if na_count > 0:\n        print(f\"  NA values: {na_count:,} ({na_count/len(train_data)*100:.1f}%)\")\n        \n    # Visualization\n    plt.figure(figsize=(8, 6))\n    plt.pie(label_20_counts.values, labels=['Down (0)', 'Up (1)'], autopct='%1.1f%%', startangle=90)\n    plt.title('Training Data: 20-day Up/Down Distribution')\n    plt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Check ret20 distribution\nif 'ret20' in train_data.columns:\n    ret20_stats = train_data['ret20'].describe()\n    print(f\"\\n20-day actual returns (ret20) statistics:\")\n    print(f\"  Mean: {ret20_stats['mean']:.4f} ({ret20_stats['mean']*100:.2f}%)\")\n    print(f\"  Std: {ret20_stats['std']:.4f} ({ret20_stats['std']*100:.2f}%)\")\n    print(f\"  Median: {ret20_stats['50%']:.4f} ({ret20_stats['50%']*100:.2f}%)\")\n    print(f\"  Min: {ret20_stats['min']:.4f} ({ret20_stats['min']*100:.2f}%)\")\n    print(f\"  Max: {ret20_stats['max']:.4f} ({ret20_stats['max']*100:.2f}%)\")\n    \n    # Return distribution histogram\n    plt.figure(figsize=(12, 5))\n    \n    plt.subplot(1, 2, 1)\n    plt.hist(train_data['ret20'].dropna(), bins=100, alpha=0.7, edgecolor='black')\n    plt.xlabel('20-day Returns')\n    plt.ylabel('Frequency')\n    plt.title('Training Data: 20-day Return Distribution')\n    plt.axvline(0, color='red', linestyle='--', alpha=0.7, label='0% baseline')\n    plt.legend()\n    \n    plt.subplot(1, 2, 2)\n    # Remove outliers and plot again\n    ret20_filtered = train_data['ret20'].dropna()\n    q1, q99 = ret20_filtered.quantile([0.01, 0.99])\n    ret20_filtered = ret20_filtered[(ret20_filtered >= q1) & (ret20_filtered <= q99)]\n    plt.hist(ret20_filtered, bins=100, alpha=0.7, edgecolor='black')\n    plt.xlabel('20-day Returns (1-99% range)')\n    plt.ylabel('Frequency')\n    plt.title('Training Data: 20-day Return Distribution (outliers removed)')\n    plt.axvline(0, color='red', linestyle='--', alpha=0.7, label='0% baseline')\n    plt.legend()\n    \n    plt.tight_layout()\n    plt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Test Data Analysis (2001-2019)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load test data\nprint(\"=== Test Data (2001-2019) ===\")\ntest_data = pd.read_parquet('data/data_2001_2019_test.parquet')\n\nprint(f\"Total data size: {len(test_data):,}\")\nprint(f\"Columns: {list(test_data.columns)}\")\nprint(f\"Date range: {test_data['date'].min()} ~ {test_data['date'].max()}\")\nprint(f\"Unique stocks: {test_data['code'].nunique():,}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 20-day up/down distribution (label_20)\nif 'label_20' in test_data.columns:\n    label_20_counts_test = test_data['label_20'].value_counts().sort_index()\n    print(f\"\\n20-day up/down distribution (test data):\")\n    print(f\"  Down (0): {label_20_counts_test.get(0, 0):,} ({label_20_counts_test.get(0, 0)/len(test_data)*100:.1f}%)\")\n    print(f\"  Up (1): {label_20_counts_test.get(1, 0):,} ({label_20_counts_test.get(1, 0)/len(test_data)*100:.1f}%)\")\n    \n    # Check NA values\n    na_count_test = test_data['label_20'].isna().sum()\n    if na_count_test > 0:\n        print(f\"  NA values: {na_count_test:,} ({na_count_test/len(test_data)*100:.1f}%)\")\n        \n    # Visualization\n    plt.figure(figsize=(8, 6))\n    plt.pie(label_20_counts_test.values, labels=['Down (0)', 'Up (1)'], autopct='%1.1f%%', startangle=90)\n    plt.title('Test Data: 20-day Up/Down Distribution')\n    plt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test data ret20 distribution check\nif 'ret20' in test_data.columns:\n    ret20_stats_test = test_data['ret20'].describe()\n    print(f\"\\n20-day actual returns (ret20) statistics (test):\")\n    print(f\"  Mean: {ret20_stats_test['mean']:.4f} ({ret20_stats_test['mean']*100:.2f}%)\")\n    print(f\"  Std: {ret20_stats_test['std']:.4f} ({ret20_stats_test['std']*100:.2f}%)\")\n    print(f\"  Median: {ret20_stats_test['50%']:.4f} ({ret20_stats_test['50%']*100:.2f}%)\")\n    print(f\"  Min: {ret20_stats_test['min']:.4f} ({ret20_stats_test['min']*100:.2f}%)\")\n    print(f\"  Max: {ret20_stats_test['max']:.4f} ({ret20_stats_test['max']*100:.2f}%)\")\n    \n    # Return distribution histogram\n    plt.figure(figsize=(12, 5))\n    \n    plt.subplot(1, 2, 1)\n    plt.hist(test_data['ret20'].dropna(), bins=100, alpha=0.7, edgecolor='black')\n    plt.xlabel('20-day Returns')\n    plt.ylabel('Frequency')\n    plt.title('Test Data: 20-day Return Distribution')\n    plt.axvline(0, color='red', linestyle='--', alpha=0.7, label='0% baseline')\n    plt.legend()\n    \n    plt.subplot(1, 2, 2)\n    # Remove outliers and plot again\n    ret20_filtered_test = test_data['ret20'].dropna()\n    q1_test, q99_test = ret20_filtered_test.quantile([0.01, 0.99])\n    ret20_filtered_test = ret20_filtered_test[(ret20_filtered_test >= q1_test) & (ret20_filtered_test <= q99_test)]\n    plt.hist(ret20_filtered_test, bins=100, alpha=0.7, edgecolor='black')\n    plt.xlabel('20-day Returns (1-99% range)')\n    plt.ylabel('Frequency')\n    plt.title('Test Data: 20-day Return Distribution (outliers removed)')\n    plt.axvline(0, color='red', linestyle='--', alpha=0.7, label='0% baseline')\n    plt.legend()\n    \n    plt.tight_layout()\n    plt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Overall Data Comparison"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Training vs test data comparison\nprint(\"=== Overall Dataset Comparison ===\")\nprint(f\"Training data (1993-2000): {len(train_data):,}\")\nprint(f\"Test data (2001-2019): {len(test_data):,}\")\nprint(f\"Total data: {len(train_data) + len(test_data):,}\")\n\nif 'label_20' in train_data.columns and 'label_20' in test_data.columns:\n    # Overall up/down distribution\n    total_up_train = label_20_counts.get(1, 0)\n    total_down_train = label_20_counts.get(0, 0)\n    total_up_test = label_20_counts_test.get(1, 0)\n    total_down_test = label_20_counts_test.get(0, 0)\n    \n    total_up = total_up_train + total_up_test\n    total_down = total_down_train + total_down_test\n    total_samples = total_up + total_down\n    \n    print(f\"\\nOverall 20-day up/down distribution:\")\n    print(f\"  Total down (0): {total_down:,} ({total_down/total_samples*100:.1f}%)\")\n    print(f\"  Total up (1): {total_up:,} ({total_up/total_samples*100:.1f}%)\")\n    \n    # Comparison bar chart\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n    \n    # Training vs test comparison\n    categories = ['Training Data\\n(1993-2000)', 'Test Data\\n(2001-2019)']\n    up_counts = [total_up_train, total_up_test]\n    down_counts = [total_down_train, total_down_test]\n    \n    x = np.arange(len(categories))\n    width = 0.35\n    \n    ax1.bar(x - width/2, down_counts, width, label='Down (0)', alpha=0.8)\n    ax1.bar(x + width/2, up_counts, width, label='Up (1)', alpha=0.8)\n    ax1.set_xlabel('Dataset')\n    ax1.set_ylabel('Number of Stocks')\n    ax1.set_title('20-day Up/Down Distribution by Dataset')\n    ax1.set_xticks(x)\n    ax1.set_xticklabels(categories)\n    ax1.legend()\n    \n    # Overall distribution pie chart\n    ax2.pie([total_down, total_up], labels=['Down (0)', 'Up (1)'], autopct='%1.1f%%', startangle=90)\n    ax2.set_title('Overall Data: 20-day Up/Down Distribution')\n    \n    plt.tight_layout()\n    plt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Compare 5-day/60-day distributions alongside"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Compare 5-day, 20-day, 60-day prediction period up/down distributions\nperiods = [5, 20, 60]\nfig, axes = plt.subplots(2, 3, figsize=(18, 10))\n\nfor i, period in enumerate(periods):\n    label_col = f'label_{period}'\n    \n    if label_col in train_data.columns:\n        # Training data\n        train_counts = train_data[label_col].value_counts().sort_index()\n        axes[0, i].pie(train_counts.values, labels=['Down (0)', 'Up (1)'], autopct='%1.1f%%', startangle=90)\n        axes[0, i].set_title(f'Training Data: {period}-day Distribution')\n        \n    if label_col in test_data.columns:\n        # Test data  \n        test_counts = test_data[label_col].value_counts().sort_index()\n        axes[1, i].pie(test_counts.values, labels=['Down (0)', 'Up (1)'], autopct='%1.1f%%', startangle=90)\n        axes[1, i].set_title(f'Test Data: {period}-day Distribution')\n\nplt.tight_layout()\nplt.show()\n\n# Numerical summary table\nsummary_data = []\nfor period in periods:\n    label_col = f'label_{period}'\n    \n    if label_col in train_data.columns:\n        train_counts = train_data[label_col].value_counts().sort_index()\n        train_up_pct = train_counts.get(1, 0) / len(train_data) * 100\n    else:\n        train_up_pct = 0\n        \n    if label_col in test_data.columns:\n        test_counts = test_data[label_col].value_counts().sort_index()\n        test_up_pct = test_counts.get(1, 0) / len(test_data) * 100\n    else:\n        test_up_pct = 0\n    \n    summary_data.append({\n        'Prediction Period': f'{period} days',\n        'Training Up %': f'{train_up_pct:.1f}%',\n        'Test Up %': f'{test_up_pct:.1f}%'\n    })\n\nsummary_df = pd.DataFrame(summary_data)\nprint(\"\\n=== Up Percentage Summary by Prediction Period ===\")\nprint(summary_df.to_string(index=False))"
  },
  {
   "cell_type": "markdown",
   "source": "## 5. Data Version Comparison: Original vs Filled Missing Values\n\nCompare the two data versions to understand the impact of filling missing open prices with previous day's close prices.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Load filled data versions (if they exist)\nimport os\n\nfilled_train_path = 'data/data_1993_2000_train_val_filled.parquet'\nfilled_test_path = 'data/data_2001_2019_test_filled.parquet'\n\nif os.path.exists(filled_train_path) and os.path.exists(filled_test_path):\n    print(\"=== Loading Filled Data Versions ===\")\n    train_data_filled = pd.read_parquet(filled_train_path)\n    test_data_filled = pd.read_parquet(filled_test_path)\n    \n    print(f\"Filled training data: {len(train_data_filled):,}\")\n    print(f\"Filled test data: {len(test_data_filled):,}\")\n    \n    # Compare missing values between original and filled versions\n    print(\"\\n=== Missing Value Comparison ===\")\n    key_columns = ['open', 'high', 'low', 'close', 'volume']\n    \n    comparison_data = []\n    for col in key_columns:\n        if col in train_data.columns and col in train_data_filled.columns:\n            orig_missing = train_data[col].isnull().sum()\n            filled_missing = train_data_filled[col].isnull().sum()\n            \n            comparison_data.append({\n                'Column': col,\n                'Original Missing': orig_missing,\n                'Filled Missing': filled_missing,\n                'Reduction': orig_missing - filled_missing,\n                'Reduction %': f\"{(orig_missing - filled_missing) / orig_missing * 100:.1f}%\" if orig_missing > 0 else \"0.0%\"\n            })\n    \n    comparison_df = pd.DataFrame(comparison_data)\n    print(\"\\nTraining Data Missing Value Comparison:\")\n    print(comparison_df.to_string(index=False))\n    \n    # Test data comparison\n    comparison_data_test = []\n    for col in key_columns:\n        if col in test_data.columns and col in test_data_filled.columns:\n            orig_missing = test_data[col].isnull().sum()\n            filled_missing = test_data_filled[col].isnull().sum()\n            \n            comparison_data_test.append({\n                'Column': col,\n                'Original Missing': orig_missing,\n                'Filled Missing': filled_missing,\n                'Reduction': orig_missing - filled_missing,\n                'Reduction %': f\"{(orig_missing - filled_missing) / orig_missing * 100:.1f}%\" if orig_missing > 0 else \"0.0%\"\n            })\n    \n    comparison_df_test = pd.DataFrame(comparison_data_test)\n    print(\"\\nTest Data Missing Value Comparison:\")\n    print(comparison_df_test.to_string(index=False))\n    \nelse:\n    print(\"⚠️  Filled data versions not found.\")\n    print(\"Please run data_preprocessing_filled.ipynb first to generate filled datasets.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 6. Model Performance Comparison\n\nCompare CNN model performance between original and filled data versions.",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}